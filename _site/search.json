[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#github-repository",
    "href": "posts/Environmental Machine Learning Project/index.html#github-repository",
    "title": "Environmental Data Project",
    "section": "GitHub Repository",
    "text": "GitHub Repository\nHere’s a link to the GitHub repo"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#dataset",
    "href": "posts/Environmental Machine Learning Project/index.html#dataset",
    "title": "Environmental Data Project",
    "section": "Dataset",
    "text": "Dataset\nBefore choosing a specific topic, I did data exploration for various datasets to see which would be the most interesting. I looked at: - US Census https://github.com/zykls/folktablesLinks - Covid19 - https://www.kaggle.com/imdevskp/covid-19-analysis-visualization-comparisons/dataLink - Uber: https://www.kaggle.com/datasets/yasserh/uber-fares-dataset - NYC: https://data.cityofnewyork.us/browse?category=Transportation"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#environmental-data-analysis",
    "href": "posts/Environmental Machine Learning Project/index.html#environmental-data-analysis",
    "title": "Environmental Data Project",
    "section": "Environmental Data Analysis",
    "text": "Environmental Data Analysis\n\n“How have land use, air quality, disasters, and climate variables changed over time? What factors are related to this?”\nCreated an account with Public EM-DAT to access a global database on natural and technological disasters\n\nConfigured parameters to obtain custom dataset\n\n\n\nimport pandas as pd\n\ndf = pd.read_excel(\"emdat.xlsx\", engine=\"openpyxl\")  \n\nprint(df.head())\n\n          DisNo. Historic Classification Key Disaster Group Disaster Subgroup  \\\n0  1999-9388-DJI       No    nat-cli-dro-dro        Natural    Climatological   \n1  1999-9388-SDN       No    nat-cli-dro-dro        Natural    Climatological   \n2  1999-9388-SOM       No    nat-cli-dro-dro        Natural    Climatological   \n3  2000-0001-AGO       No    tec-tra-roa-roa  Technological         Transport   \n4  2000-0002-AGO       No    nat-hyd-flo-riv        Natural      Hydrological   \n\n  Disaster Type Disaster Subtype External IDs Event Name  ISO  ...  \\\n0       Drought          Drought          NaN        NaN  DJI  ...   \n1       Drought          Drought          NaN        NaN  SDN  ...   \n2       Drought          Drought          NaN        NaN  SOM  ...   \n3          Road             Road          NaN        NaN  AGO  ...   \n4         Flood   Riverine flood          NaN        NaN  AGO  ...   \n\n  Reconstruction Costs ('000 US$) Reconstruction Costs, Adjusted ('000 US$)  \\\n0                             NaN                                       NaN   \n1                             NaN                                       NaN   \n2                             NaN                                       NaN   \n3                             NaN                                       NaN   \n4                             NaN                                       NaN   \n\n  Insured Damage ('000 US$) Insured Damage, Adjusted ('000 US$)  \\\n0                       NaN                                 NaN   \n1                       NaN                                 NaN   \n2                       NaN                                 NaN   \n3                       NaN                                 NaN   \n4                       NaN                                 NaN   \n\n  Total Damage ('000 US$) Total Damage, Adjusted ('000 US$)        CPI  \\\n0                     NaN                               NaN  58.111474   \n1                     NaN                               NaN  56.514291   \n2                     NaN                               NaN  56.514291   \n3                     NaN                               NaN  56.514291   \n4                 10000.0                           17695.0  56.514291   \n\n                                         Admin Units  Entry Date  Last Update  \n0  [{\"adm1_code\":1093,\"adm1_name\":\"Ali Sabieh\"},{...  2006-03-01   2023-09-25  \n1  [{\"adm1_code\":2757,\"adm1_name\":\"Northern Darfu...  2006-03-08   2023-09-25  \n2  [{\"adm1_code\":2691,\"adm1_name\":\"Bay\"},{\"adm1_c...  2006-03-08   2023-09-25  \n3                                                NaN  2004-10-27   2023-09-25  \n4  [{\"adm2_code\":4214,\"adm2_name\":\"Baia Farta\"},{...  2005-02-03   2023-09-25  \n\n[5 rows x 46 columns]\n\n\nLet’s inspect the data\n\ndf.info()\ndf.describe()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 16209 entries, 0 to 16208\nData columns (total 46 columns):\n #   Column                                     Non-Null Count  Dtype  \n---  ------                                     --------------  -----  \n 0   DisNo.                                     16209 non-null  object \n 1   Historic                                   16209 non-null  object \n 2   Classification Key                         16209 non-null  object \n 3   Disaster Group                             16209 non-null  object \n 4   Disaster Subgroup                          16209 non-null  object \n 5   Disaster Type                              16209 non-null  object \n 6   Disaster Subtype                           16209 non-null  object \n 7   External IDs                               2709 non-null   object \n 8   Event Name                                 5105 non-null   object \n 9   ISO                                        16209 non-null  object \n 10  Country                                    16209 non-null  object \n 11  Subregion                                  16209 non-null  object \n 12  Region                                     16209 non-null  object \n 13  Location                                   15499 non-null  object \n 14  Origin                                     4076 non-null   object \n 15  Associated Types                           3438 non-null   object \n 16  OFDA/BHA Response                          16209 non-null  object \n 17  Appeal                                     16209 non-null  object \n 18  Declaration                                16209 non-null  object \n 19  AID Contribution ('000 US$)                489 non-null    float64\n 20  Magnitude                                  3311 non-null   float64\n 21  Magnitude Scale                            10137 non-null  object \n 22  Latitude                                   1816 non-null   float64\n 23  Longitude                                  1816 non-null   float64\n 24  River Basin                                1233 non-null   object \n 25  Start Year                                 16209 non-null  int64  \n 26  Start Month                                16140 non-null  float64\n 27  Start Day                                  14638 non-null  float64\n 28  End Year                                   16209 non-null  int64  \n 29  End Month                                  16046 non-null  float64\n 30  End Day                                    14710 non-null  float64\n 31  Total Deaths                               13039 non-null  float64\n 32  No. Injured                                6019 non-null   float64\n 33  No. Affected                               7503 non-null   float64\n 34  No. Homeless                               1333 non-null   float64\n 35  Total Affected                             12087 non-null  float64\n 36  Reconstruction Costs ('000 US$)            33 non-null     float64\n 37  Reconstruction Costs, Adjusted ('000 US$)  33 non-null     float64\n 38  Insured Damage ('000 US$)                  713 non-null    float64\n 39  Insured Damage, Adjusted ('000 US$)        694 non-null    float64\n 40  Total Damage ('000 US$)                    3237 non-null   float64\n 41  Total Damage, Adjusted ('000 US$)          3110 non-null   float64\n 42  CPI                                        15527 non-null  float64\n 43  Admin Units                                8416 non-null   object \n 44  Entry Date                                 16209 non-null  object \n 45  Last Update                                16209 non-null  object \ndtypes: float64(20), int64(2), object(24)\nmemory usage: 5.7+ MB\n\n\n\n\n\n\n\n\n\nAID Contribution ('000 US$)\nMagnitude\nLatitude\nLongitude\nStart Year\nStart Month\nStart Day\nEnd Year\nEnd Month\nEnd Day\n...\nNo. Affected\nNo. Homeless\nTotal Affected\nReconstruction Costs ('000 US$)\nReconstruction Costs, Adjusted ('000 US$)\nInsured Damage ('000 US$)\nInsured Damage, Adjusted ('000 US$)\nTotal Damage ('000 US$)\nTotal Damage, Adjusted ('000 US$)\nCPI\n\n\n\n\ncount\n4.890000e+02\n3.311000e+03\n1816.000000\n1816.000000\n16209.000000\n16140.000000\n14638.000000\n16209.000000\n16046.000000\n14710.000000\n...\n7.503000e+03\n1.333000e+03\n1.208700e+04\n3.300000e+01\n3.300000e+01\n7.130000e+02\n6.940000e+02\n3.237000e+03\n3.110000e+03\n15527.000000\n\n\nmean\n2.855916e+04\n6.122545e+04\n16.415862\n42.477620\n2011.109816\n6.463755\n15.352849\n2011.141156\n6.592858\n15.814276\n...\n6.280205e+05\n3.162704e+04\n3.939913e+05\n5.687264e+06\n6.357118e+06\n1.347736e+06\n1.699765e+06\n1.178820e+06\n1.478426e+06\n72.858610\n\n\nstd\n2.118956e+05\n7.486415e+05\n21.786044\n75.523526\n7.422845\n3.413559\n8.973253\n7.425922\n3.391621\n8.891107\n...\n6.649927e+06\n2.143536e+05\n5.249462e+06\n1.745232e+07\n1.760343e+07\n4.644761e+06\n5.954430e+06\n6.317104e+06\n8.316465e+06\n11.582942\n\n\nmin\n3.000000e+00\n-5.700000e+01\n-72.640000\n-172.095000\n2000.000000\n1.000000\n1.000000\n2000.000000\n1.000000\n1.000000\n...\n1.000000e+00\n3.000000e+00\n1.000000e+00\n8.400000e+01\n1.310000e+02\n3.400000e+01\n4.800000e+01\n2.000000e+00\n3.000000e+00\n56.514291\n\n\n25%\n1.660000e+02\n2.350000e+01\n1.061500\n1.676500\n2005.000000\n4.000000\n7.000000\n2005.000000\n4.000000\n8.000000\n...\n6.000000e+02\n3.400000e+02\n4.200000e+01\n1.000000e+05\n1.000000e+05\n7.500000e+04\n9.917050e+04\n1.600000e+04\n2.132325e+04\n61.989586\n\n\n50%\n7.650000e+02\n2.000000e+02\n18.642500\n55.574500\n2010.000000\n7.000000\n15.000000\n2010.000000\n7.000000\n16.000000\n...\n6.500000e+03\n1.966000e+03\n1.000000e+03\n5.650000e+05\n7.023360e+05\n2.500000e+05\n3.491245e+05\n1.000000e+05\n1.420280e+05\n71.563596\n\n\n75%\n4.984000e+03\n2.173700e+04\n34.786750\n103.235250\n2018.000000\n9.000000\n23.000000\n2018.000000\n9.000000\n24.000000\n...\n6.003500e+04\n7.000000e+03\n1.757050e+04\n3.344000e+06\n4.245383e+06\n8.000000e+05\n1.117445e+06\n5.500000e+05\n7.172402e+05\n80.445779\n\n\nmax\n3.518530e+06\n4.000000e+07\n67.930000\n179.650000\n2025.000000\n12.000000\n31.000000\n2025.000000\n12.000000\n31.000000\n...\n3.300000e+08\n5.000000e+06\n3.300000e+08\n1.000000e+08\n1.000000e+08\n6.000000e+07\n9.361435e+07\n2.100000e+08\n2.844652e+08\n100.000000\n\n\n\n\n8 rows × 22 columns"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#data-cleaning",
    "href": "posts/Environmental Machine Learning Project/index.html#data-cleaning",
    "title": "Environmental Data Project",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nGet rid of columns where the data is irrelevant or there’s not enough of it for it to be interesting.\n\ndf.isna().sum().sort_values(ascending=False)\n\nReconstruction Costs, Adjusted ('000 US$)    16176\nReconstruction Costs ('000 US$)              16176\nAID Contribution ('000 US$)                  15720\nInsured Damage, Adjusted ('000 US$)          15515\nInsured Damage ('000 US$)                    15496\nRiver Basin                                  14976\nNo. Homeless                                 14876\nLongitude                                    14393\nLatitude                                     14393\nExternal IDs                                 13500\nTotal Damage, Adjusted ('000 US$)            13099\nTotal Damage ('000 US$)                      12972\nMagnitude                                    12898\nAssociated Types                             12771\nOrigin                                       12133\nEvent Name                                   11104\nNo. Injured                                  10190\nNo. Affected                                  8706\nAdmin Units                                   7793\nMagnitude Scale                               6072\nTotal Affected                                4122\nTotal Deaths                                  3170\nStart Day                                     1571\nEnd Day                                       1499\nLocation                                       710\nCPI                                            682\nEnd Month                                      163\nStart Month                                     69\nEntry Date                                       0\nDisNo.                                           0\nEnd Year                                         0\nStart Year                                       0\nHistoric                                         0\nDeclaration                                      0\nAppeal                                           0\nOFDA/BHA Response                                0\nRegion                                           0\nSubregion                                        0\nCountry                                          0\nISO                                              0\nDisaster Subtype                                 0\nDisaster Type                                    0\nDisaster Subgroup                                0\nDisaster Group                                   0\nClassification Key                               0\nLast Update                                      0\ndtype: int64\n\n\n\ndf = df.drop(columns=[ 'DisNo.', 'External IDs','Event Name', 'Origin','Associated Types','Appeal','Declaration','OFDA/BHA Response','AID Contribution (\\'000 US$)',  'Reconstruction Costs (\\'000 US$)','Reconstruction Costs, Adjusted (\\'000 US$)','Insured Damage (\\'000 US$)','Insured Damage, Adjusted (\\'000 US$)','River Basin','Admin Units','Entry Date','Last Update'   ], errors='ignore')\ndf\n\n\n\n\n\n\n\n\nHistoric\nClassification Key\nDisaster Group\nDisaster Subgroup\nDisaster Type\nDisaster Subtype\nISO\nCountry\nSubregion\nRegion\n...\nEnd Month\nEnd Day\nTotal Deaths\nNo. Injured\nNo. Affected\nNo. Homeless\nTotal Affected\nTotal Damage ('000 US$)\nTotal Damage, Adjusted ('000 US$)\nCPI\n\n\n\n\n0\nNo\nnat-cli-dro-dro\nNatural\nClimatological\nDrought\nDrought\nDJI\nDjibouti\nSub-Saharan Africa\nAfrica\n...\nNaN\nNaN\nNaN\nNaN\n100000.0\nNaN\n100000.0\nNaN\nNaN\n58.111474\n\n\n1\nNo\nnat-cli-dro-dro\nNatural\nClimatological\nDrought\nDrought\nSDN\nSudan\nNorthern Africa\nAfrica\n...\nNaN\nNaN\nNaN\nNaN\n2000000.0\nNaN\n2000000.0\nNaN\nNaN\n56.514291\n\n\n2\nNo\nnat-cli-dro-dro\nNatural\nClimatological\nDrought\nDrought\nSOM\nSomalia\nSub-Saharan Africa\nAfrica\n...\nNaN\nNaN\n21.0\nNaN\n1200000.0\nNaN\n1200000.0\nNaN\nNaN\n56.514291\n\n\n3\nNo\ntec-tra-roa-roa\nTechnological\nTransport\nRoad\nRoad\nAGO\nAngola\nSub-Saharan Africa\nAfrica\n...\n1.0\n26.0\n14.0\n11.0\nNaN\nNaN\n11.0\nNaN\nNaN\n56.514291\n\n\n4\nNo\nnat-hyd-flo-riv\nNatural\nHydrological\nFlood\nRiverine flood\nAGO\nAngola\nSub-Saharan Africa\nAfrica\n...\n1.0\n15.0\n31.0\nNaN\n70000.0\nNaN\n70000.0\n10000.0\n17695.0\n56.514291\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16204\nNo\ntec-tra-roa-roa\nTechnological\nTransport\nRoad\nRoad\nBRA\nBrazil\nLatin America and the Caribbean\nAmericas\n...\n4.0\n8.0\n10.0\n18.0\nNaN\nNaN\n18.0\nNaN\nNaN\nNaN\n\n\n16205\nNo\ntec-mis-fir-fir\nTechnological\nMiscellaneous accident\nFire (Miscellaneous)\nFire (Miscellaneous)\nCHN\nChina\nEastern Asia\nAsia\n...\n4.0\n8.0\n20.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n16206\nNo\nnat-met-sto-san\nNatural\nMeteorological\nStorm\nSand/Dust storm\nIRQ\nIraq\nWestern Asia\nAsia\n...\n4.0\n14.0\nNaN\n2751.0\nNaN\nNaN\n2751.0\nNaN\nNaN\nNaN\n\n\n16207\nNo\nnat-met-sto-sto\nNatural\nMeteorological\nStorm\nStorm (General)\nSPI\nCanary Islands\nNorthern Africa\nAfrica\n...\n4.0\n13.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n16208\nNo\nnat-cli-dro-dro\nNatural\nClimatological\nDrought\nDrought\nSOM\nSomalia\nSub-Saharan Africa\nAfrica\n...\nNaN\nNaN\nNaN\nNaN\n4400000.0\nNaN\n4400000.0\nNaN\nNaN\nNaN\n\n\n\n\n16209 rows × 29 columns\n\n\n\nLet’s also get rid of data points if it’s missing important variables we care about.\n\ndf = df.dropna(subset=['Country', 'Disaster Type', 'Start Year'])\ndf\n\n\n\n\n\n\n\n\nHistoric\nClassification Key\nDisaster Group\nDisaster Subgroup\nDisaster Type\nDisaster Subtype\nISO\nCountry\nSubregion\nRegion\n...\nEnd Month\nEnd Day\nTotal Deaths\nNo. Injured\nNo. Affected\nNo. Homeless\nTotal Affected\nTotal Damage ('000 US$)\nTotal Damage, Adjusted ('000 US$)\nCPI\n\n\n\n\n0\nNo\nnat-cli-dro-dro\nNatural\nClimatological\nDrought\nDrought\nDJI\nDjibouti\nSub-Saharan Africa\nAfrica\n...\nNaN\nNaN\nNaN\nNaN\n100000.0\nNaN\n100000.0\nNaN\nNaN\n58.111474\n\n\n1\nNo\nnat-cli-dro-dro\nNatural\nClimatological\nDrought\nDrought\nSDN\nSudan\nNorthern Africa\nAfrica\n...\nNaN\nNaN\nNaN\nNaN\n2000000.0\nNaN\n2000000.0\nNaN\nNaN\n56.514291\n\n\n2\nNo\nnat-cli-dro-dro\nNatural\nClimatological\nDrought\nDrought\nSOM\nSomalia\nSub-Saharan Africa\nAfrica\n...\nNaN\nNaN\n21.0\nNaN\n1200000.0\nNaN\n1200000.0\nNaN\nNaN\n56.514291\n\n\n3\nNo\ntec-tra-roa-roa\nTechnological\nTransport\nRoad\nRoad\nAGO\nAngola\nSub-Saharan Africa\nAfrica\n...\n1.0\n26.0\n14.0\n11.0\nNaN\nNaN\n11.0\nNaN\nNaN\n56.514291\n\n\n4\nNo\nnat-hyd-flo-riv\nNatural\nHydrological\nFlood\nRiverine flood\nAGO\nAngola\nSub-Saharan Africa\nAfrica\n...\n1.0\n15.0\n31.0\nNaN\n70000.0\nNaN\n70000.0\n10000.0\n17695.0\n56.514291\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16204\nNo\ntec-tra-roa-roa\nTechnological\nTransport\nRoad\nRoad\nBRA\nBrazil\nLatin America and the Caribbean\nAmericas\n...\n4.0\n8.0\n10.0\n18.0\nNaN\nNaN\n18.0\nNaN\nNaN\nNaN\n\n\n16205\nNo\ntec-mis-fir-fir\nTechnological\nMiscellaneous accident\nFire (Miscellaneous)\nFire (Miscellaneous)\nCHN\nChina\nEastern Asia\nAsia\n...\n4.0\n8.0\n20.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n16206\nNo\nnat-met-sto-san\nNatural\nMeteorological\nStorm\nSand/Dust storm\nIRQ\nIraq\nWestern Asia\nAsia\n...\n4.0\n14.0\nNaN\n2751.0\nNaN\nNaN\n2751.0\nNaN\nNaN\nNaN\n\n\n16207\nNo\nnat-met-sto-sto\nNatural\nMeteorological\nStorm\nStorm (General)\nSPI\nCanary Islands\nNorthern Africa\nAfrica\n...\n4.0\n13.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n16208\nNo\nnat-cli-dro-dro\nNatural\nClimatological\nDrought\nDrought\nSOM\nSomalia\nSub-Saharan Africa\nAfrica\n...\nNaN\nNaN\nNaN\nNaN\n4400000.0\nNaN\n4400000.0\nNaN\nNaN\nNaN\n\n\n\n\n16209 rows × 29 columns\n\n\n\nGet rid of duplicates\n\ndf = df.drop_duplicates()\ndf\n\n\n\n\n\n\n\n\nHistoric\nClassification Key\nDisaster Group\nDisaster Subgroup\nDisaster Type\nDisaster Subtype\nISO\nCountry\nSubregion\nRegion\n...\nEnd Month\nEnd Day\nTotal Deaths\nNo. Injured\nNo. Affected\nNo. Homeless\nTotal Affected\nTotal Damage ('000 US$)\nTotal Damage, Adjusted ('000 US$)\nCPI\n\n\n\n\n0\nNo\nnat-cli-dro-dro\nNatural\nClimatological\nDrought\nDrought\nDJI\nDjibouti\nSub-Saharan Africa\nAfrica\n...\nNaN\nNaN\nNaN\nNaN\n100000.0\nNaN\n100000.0\nNaN\nNaN\n58.111474\n\n\n1\nNo\nnat-cli-dro-dro\nNatural\nClimatological\nDrought\nDrought\nSDN\nSudan\nNorthern Africa\nAfrica\n...\nNaN\nNaN\nNaN\nNaN\n2000000.0\nNaN\n2000000.0\nNaN\nNaN\n56.514291\n\n\n2\nNo\nnat-cli-dro-dro\nNatural\nClimatological\nDrought\nDrought\nSOM\nSomalia\nSub-Saharan Africa\nAfrica\n...\nNaN\nNaN\n21.0\nNaN\n1200000.0\nNaN\n1200000.0\nNaN\nNaN\n56.514291\n\n\n3\nNo\ntec-tra-roa-roa\nTechnological\nTransport\nRoad\nRoad\nAGO\nAngola\nSub-Saharan Africa\nAfrica\n...\n1.0\n26.0\n14.0\n11.0\nNaN\nNaN\n11.0\nNaN\nNaN\n56.514291\n\n\n4\nNo\nnat-hyd-flo-riv\nNatural\nHydrological\nFlood\nRiverine flood\nAGO\nAngola\nSub-Saharan Africa\nAfrica\n...\n1.0\n15.0\n31.0\nNaN\n70000.0\nNaN\n70000.0\n10000.0\n17695.0\n56.514291\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16204\nNo\ntec-tra-roa-roa\nTechnological\nTransport\nRoad\nRoad\nBRA\nBrazil\nLatin America and the Caribbean\nAmericas\n...\n4.0\n8.0\n10.0\n18.0\nNaN\nNaN\n18.0\nNaN\nNaN\nNaN\n\n\n16205\nNo\ntec-mis-fir-fir\nTechnological\nMiscellaneous accident\nFire (Miscellaneous)\nFire (Miscellaneous)\nCHN\nChina\nEastern Asia\nAsia\n...\n4.0\n8.0\n20.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n16206\nNo\nnat-met-sto-san\nNatural\nMeteorological\nStorm\nSand/Dust storm\nIRQ\nIraq\nWestern Asia\nAsia\n...\n4.0\n14.0\nNaN\n2751.0\nNaN\nNaN\n2751.0\nNaN\nNaN\nNaN\n\n\n16207\nNo\nnat-met-sto-sto\nNatural\nMeteorological\nStorm\nStorm (General)\nSPI\nCanary Islands\nNorthern Africa\nAfrica\n...\n4.0\n13.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n16208\nNo\nnat-cli-dro-dro\nNatural\nClimatological\nDrought\nDrought\nSOM\nSomalia\nSub-Saharan Africa\nAfrica\n...\nNaN\nNaN\nNaN\nNaN\n4400000.0\nNaN\n4400000.0\nNaN\nNaN\nNaN\n\n\n\n\n16209 rows × 29 columns"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#interesting-variables",
    "href": "posts/Environmental Machine Learning Project/index.html#interesting-variables",
    "title": "Environmental Data Project",
    "section": "Interesting Variables:",
    "text": "Interesting Variables:\n\nDisaster Type\n\nLocation (country, region, longitude, latitude)\nTotal Deaths, No. Affected, and No. Injured\nTime (start day, start month, etc.)\nCost (Reconstruction Costs and Total Damage)"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#research-questions",
    "href": "posts/Environmental Machine Learning Project/index.html#research-questions",
    "title": "Environmental Data Project",
    "section": "Research Questions:",
    "text": "Research Questions:\n\n“Which countries have experienced the highest number of natural disasters in the past 25 years?”\n\n\nWe’ll need to investigate variables on countries, when it occured, and type of disaster\n\n\n“Are certain types of disasters becoming more common over time?”\n\n\nWe’ll need to investigate variables on time, when it occured, and type of disaster\nLet’s consider limitations with this dataset since it starts in 2000\n\n\n“Can we predict the average number of deaths or affected people for an event based on disaster type and context?”\n\n\nWe’ll need to investigate variables on Total Deaths, No. Affected, and No. Injured and type of disaster\n\n\n“Can we experiment with data forecasting and predict the number of disasters a country might experience based on its historical trends and geography?”\n\n\nWe’ll need to investigate variables on type of disaster, when it occured, and location\nWe’ll need to incorporate another dataset on geography\n\n\n“Can we classify the severity of a disaster based on its type, location, and year?”\n\n\nWe’ll need to investigate variables on type, location, time, and severity"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#tools-needed",
    "href": "posts/Environmental Machine Learning Project/index.html#tools-needed",
    "title": "Environmental Data Project",
    "section": "Tools Needed",
    "text": "Tools Needed\n\nFor visualizations, matplotlib, geopanda, seaborn and plotly\nFor machine learning, specifically forecasting, we can try Scikit-learn for regression or classification models or LSTM\nFor classification, we can use it to identify patterns or relationships among disasters"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#lets-get-started-on-first-research-question",
    "href": "posts/Environmental Machine Learning Project/index.html#lets-get-started-on-first-research-question",
    "title": "Environmental Data Project",
    "section": "Let’s get started on first research question",
    "text": "Let’s get started on first research question\n\n“Which countries have experienced the highest number of natural disasters?”\n\n\nWe’ll need to investigate variables on countries, when it occured, and type of disaster\n\n\ndisaster_count_by_country_df = df.groupby(['Country']).agg(\n    disaster_count=('Disaster Type', 'count')\n).reset_index().sort_values(by='disaster_count', ascending=False)\n\n\ndisaster_count_by_country_df\n\n\n\n\n\n\n\n\nCountry\ndisaster_count\n\n\n\n\n39\nChina\n1356\n\n\n89\nIndia\n820\n\n\n212\nUnited States of America\n724\n\n\n90\nIndonesia\n565\n\n\n152\nPhilippines\n474\n\n\n...\n...\n...\n\n\n5\nAnguilla\n1\n\n\n218\nWallis and Futuna Islands\n1\n\n\n164\nSaint Helena\n1\n\n\n135\nNetherlands Antilles\n1\n\n\n49\nCuraçao\n1\n\n\n\n\n222 rows × 2 columns\n\n\n\n\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nfig = px.scatter(\n    disaster_count_by_country_df.query('disaster_count&gt;=300'),\n    x=\"Country\",\n    y=\"disaster_count\",\n    size=\"disaster_count\",\n    color=\"Country\",\n    hover_name=\"Country\",\n    log_y=True, \n    size_max=100,\n    title=\"Number of Disaster Types by Country\"\n)\n\nfig.show()"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#analysis-of-first-research-question",
    "href": "posts/Environmental Machine Learning Project/index.html#analysis-of-first-research-question",
    "title": "Environmental Data Project",
    "section": "Analysis of first research question",
    "text": "Analysis of first research question\nFrom this visualization, we can see that India, China, and the USA have experienced the highest number of natural disasters. This is likely due to their large size, which means that there is more surface area taken into account. Surprising countries are the ones that are smaller but still have faced many natural disasters, such as the Philippines and Nigeria."
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#lets-get-started-on-the-second-research-question",
    "href": "posts/Environmental Machine Learning Project/index.html#lets-get-started-on-the-second-research-question",
    "title": "Environmental Data Project",
    "section": "Let’s get started on the second research question",
    "text": "Let’s get started on the second research question\n\n“Are certain types of disasters becoming more common over time?”\n\n\nWe’ll need to investigate variables on time, when it occured, and type of disaster\nLet’s consider limitations with this dataset since it starts in 2000\n\n\ndisaster_counts = df.groupby(['Start Year', 'Disaster Type']).size().reset_index(name='Count')\n\ndisaster_counts\n\n\n\n\n\n\n\n\nStart Year\nDisaster Type\nCount\n\n\n\n\n0\n2000\nAir\n31\n\n\n1\n2000\nChemical spill\n3\n\n\n2\n2000\nCollapse (Industrial)\n3\n\n\n3\n2000\nCollapse (Miscellaneous)\n11\n\n\n4\n2000\nDrought\n27\n\n\n...\n...\n...\n...\n\n\n609\n2025\nRail\n1\n\n\n610\n2025\nRoad\n19\n\n\n611\n2025\nStorm\n26\n\n\n612\n2025\nWater\n4\n\n\n613\n2025\nWildfire\n8\n\n\n\n\n614 rows × 3 columns\n\n\n\n\nfig = px.line(\n    disaster_counts,\n    x='Start Year',\n    y='Count',\n    color='Disaster Type',  \n    title='Disaster Types Over Time',\n    labels={'Year': 'Year', 'Count': 'Number of Disasters'},\n)\n\nfig.show()"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#analysis-of-second-research-question",
    "href": "posts/Environmental Machine Learning Project/index.html#analysis-of-second-research-question",
    "title": "Environmental Data Project",
    "section": "Analysis of second research question",
    "text": "Analysis of second research question\nFrom this visualization, we can see that it is difficult to answer if there are certain types of disasters becoming more common over time. This is because of our limited data set that starts in the year 2000. To answer, this question, we would need to find data that dates back far enough to see a difference in the frequency of certain disaster types."
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#visualization-to-explore-relationship-between-total-deaths-and-occurence-count-for-each-disaster-subtype",
    "href": "posts/Environmental Machine Learning Project/index.html#visualization-to-explore-relationship-between-total-deaths-and-occurence-count-for-each-disaster-subtype",
    "title": "Environmental Data Project",
    "section": "Visualization to explore relationship between total deaths and occurence count for each disaster subtype",
    "text": "Visualization to explore relationship between total deaths and occurence count for each disaster subtype\n\nscatter_data = df.groupby('Disaster Subtype').agg(\n    total_deaths=('Total Deaths', 'sum'),\n    occurrence_count=('Disaster Subtype', 'count')\n).reset_index()\n\nscatter_data\n\n\n\n\n\n\n\n\nDisaster Subtype\ntotal_deaths\noccurrence_count\n\n\n\n\n0\nAir\n17267.0\n439\n\n\n1\nAnimal incident\n12.0\n1\n\n\n2\nAsh fall\n663.0\n102\n\n\n3\nAvalanche (dry)\n16.0\n1\n\n\n4\nAvalanche (wet)\n1889.0\n54\n\n\n...\n...\n...\n...\n\n\n58\nViral disease\n47825.0\n350\n\n\n59\nVolcanic activity (General)\n516.0\n11\n\n\n60\nWater\n51946.0\n1149\n\n\n61\nWildfire (General)\n702.0\n85\n\n\n62\nWorms infestation\n0.0\n2\n\n\n\n\n63 rows × 3 columns\n\n\n\n\nfig = px.scatter(\n    scatter_data,\n    x='Disaster Subtype',\n    y='total_deaths',\n    size='occurrence_count',    \n    color='Disaster Subtype',     \n    title='Disaster Subtypes: Total Deaths vs. Occurrence Count',\n    labels={\n        'disaster_subtype': 'Disaster Subtype',\n        'total_deaths': 'Total Deaths'\n    },\n)\n\n\n\nfig.show()"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#analysis-of-scatterplot-visualization",
    "href": "posts/Environmental Machine Learning Project/index.html#analysis-of-scatterplot-visualization",
    "title": "Environmental Data Project",
    "section": "Analysis of scatterplot visualization",
    "text": "Analysis of scatterplot visualization\nWe can see that Ground Movement is an outlier and has the highest number of deaths at around 550k. In terms of disaster subtypes with the largest occurence count, we can notice Road, Riverine Flood, Flood, Water, and Tropical Cyclone. We can see that most of these are natural with the exception of Road."
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#map-visualization-on-devastating-disasters-around-the-world",
    "href": "posts/Environmental Machine Learning Project/index.html#map-visualization-on-devastating-disasters-around-the-world",
    "title": "Environmental Data Project",
    "section": "Map Visualization on Devastating Disasters Around the World",
    "text": "Map Visualization on Devastating Disasters Around the World\n\nmap_df = df[(df['Latitude'].notna()) & (df['Longitude'].notna())]\n\nmap_df['Total Deaths'] = map_df['Total Deaths'].fillna(0)\n\nmap_df = map_df[(map_df['Total Deaths'] &gt; 0)]\n\nmap_df\n\n/var/folders/h6/1nj6sx2s1nxdyfr36bhcxvlc0000gn/T/ipykernel_50407/904054904.py:3: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\n\n\n\nHistoric\nClassification Key\nDisaster Group\nDisaster Subgroup\nDisaster Type\nDisaster Subtype\nISO\nCountry\nSubregion\nRegion\n...\nEnd Month\nEnd Day\nTotal Deaths\nNo. Injured\nNo. Affected\nNo. Homeless\nTotal Affected\nTotal Damage ('000 US$)\nTotal Damage, Adjusted ('000 US$)\nCPI\n\n\n\n\n33\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nCHN\nChina\nEastern Asia\nAsia\n...\n1.0\n14.0\n7.0\n2528.0\n1760000.0\n92479.0\n1855007.0\n73500.0\n130056.0\n56.514291\n\n\n36\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nCHN\nChina\nEastern Asia\nAsia\n...\n1.0\n26.0\n1.0\n2.0\n10300.0\nNaN\n10302.0\n483.0\n855.0\n56.514291\n\n\n50\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nIRN\nIran (Islamic Republic of)\nSouthern Asia\nAsia\n...\n2.0\n2.0\n1.0\n15.0\n1500.0\n500.0\n2015.0\nNaN\nNaN\n56.514291\n\n\n75\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nCOL\nColombia\nLatin America and the Caribbean\nAmericas\n...\n11.0\n8.0\n2.0\nNaN\n430.0\nNaN\n430.0\nNaN\nNaN\n56.514291\n\n\n208\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nIDN\nIndonesia\nSouth-eastern Asia\nAsia\n...\n5.0\n4.0\n45.0\n270.0\nNaN\n52500.0\n52770.0\n30000.0\n53084.0\n56.514291\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16091\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nCHN\nChina\nEastern Asia\nAsia\n...\n1.0\n7.0\n126.0\n188.0\n46500.0\nNaN\n46688.0\nNaN\nNaN\nNaN\n\n\n16099\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nETH\nEthiopia\nSub-Saharan Africa\nAfrica\n...\n1.0\n11.0\n2.0\nNaN\n99000.0\nNaN\n99000.0\nNaN\nNaN\nNaN\n\n\n16184\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nMMR\nMyanmar\nSouth-eastern Asia\nAsia\n...\n3.0\n28.0\n3784.0\n4824.0\n282790.0\nNaN\n287614.0\nNaN\nNaN\nNaN\n\n\n16185\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nTHA\nThailand\nSouth-eastern Asia\nAsia\n...\n3.0\n28.0\n44.0\n37.0\n2313.0\nNaN\n2350.0\nNaN\nNaN\nNaN\n\n\n16196\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nTJK\nTajikistan\nCentral Asia\nAsia\n...\n4.0\n13.0\n1.0\nNaN\n145.0\nNaN\n145.0\nNaN\nNaN\nNaN\n\n\n\n\n1356 rows × 29 columns\n\n\n\n\nfig = px.scatter_geo(\n    map_df,\n    lat='Latitude',\n    lon='Longitude',\n    color='Disaster Subtype',\n    size='Total Deaths',  \n    hover_name='Country',\n    title='Global Disasters Map',\n    size_max=30,\n)\n\nfig.update_layout(\n    geo=dict(\n        showland=True,\n        showcountries=True\n    )\n)\n\nfig.show()"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#analysis-of-map-visualization",
    "href": "posts/Environmental Machine Learning Project/index.html#analysis-of-map-visualization",
    "title": "Environmental Data Project",
    "section": "Analysis of Map Visualization",
    "text": "Analysis of Map Visualization\nHere, we can see that Haiti experienced a Ground Movement disaster that resulted in a huge amount of deaths. We also see that Indonesia has deathly tsunamis. China, Kazakhstan, and Russia face a lot of floods that have caused a large loss of lives."
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#responding-to-questions-from-research-professor-prior-week",
    "href": "posts/Environmental Machine Learning Project/index.html#responding-to-questions-from-research-professor-prior-week",
    "title": "Environmental Data Project",
    "section": "Responding to questions from research professor prior week",
    "text": "Responding to questions from research professor prior week\n\n“This is likely due to their large size, which means that there is more surface area taken into account.” -&gt; Would it be possible to apply some normalization techniques to account for differences in country size? -&gt; answer: Great idea! We could try to find another data source that contains the country size and use data techniques to merge it together. Once we’ve prepared the data, we can perform normalization techniques by dividing the number of natural disasters with the corresponding country’s size. We can try doing this during week 5 when we incorporate SQL.\nWould you consider numbering the research questions with bullet points to make them easier to reference? -&gt; Yes, I’ll go back and adjust them! This is a great piece of advice\nClarifying question “dataset spans from 2000 to 2025—is that right?” -&gt; answer: Yes, this dataset spans 25 years starting from 2000. Even though there exists data before 2000, according to the data source, “Pre-2000 data is particularly subject to reporting biases.”\nRegarding visualizations and figures: Could you ensure consistency in how parameters are represented? For example, if size represents occurrence or death, it would be helpful to maintain that convention across all figures. -&gt; answer: Yes, this is a helpful piece of advice but the reason for the inconsistency in how parameters are represented is due to how these visualizations were more so for exploratory purposes to better understand the data. I’ll keep this advice in mind and ensure consistency in how parameters are represented for the final report."
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#air-pollution-dataset-using-api",
    "href": "posts/Environmental Machine Learning Project/index.html#air-pollution-dataset-using-api",
    "title": "Environmental Data Project",
    "section": "Air Pollution Dataset Using API",
    "text": "Air Pollution Dataset Using API\nOur data source is called Open Weather Map. I made an account with them and verified my email address to obtain the API key. According to the website, “the Air Pollution API provides current, forecast and historical air pollution data for any coordinates on the globe. Besides basic Air Quality Index, the API returns data about polluting gases, such as Carbon monoxide (CO), Nitrogen monoxide (NO), Nitrogen dioxide (NO2), Ozone (O3), Sulphur dioxide (SO2), Ammonia (NH3), and particulates (PM2.5 and PM10). Air pollution forecast is available for 4 days with hourly granularity. Historical data is accessible from 27th November 2020.”\n\n\n\n\nimport requests\n\nAPI_KEY = \"dc622205c25e4f765124ba4c03f370ba\"\nLAT = 35.6764\nLON = 139.6500\n\nurl = f\"http://api.openweathermap.org/data/2.5/air_pollution\"\nparams = {\n    \"lat\": LAT,\n    \"lon\": LON,\n    \"appid\": API_KEY\n}\n\nresponse = requests.get(url, params=params)\ndata = response.json()\n\nprint(\"Air Quality Index (AQI):\", data['list'][0]['main']['aqi'])\nprint(\"Components (μg/m³):\", data['list'][0]['components'])\n\nAir Quality Index (AQI): 1\nComponents (μg/m³): {'co': 173.01, 'no': 3.6, 'no2': 25.53, 'o3': 50.95, 'so2': 5.56, 'pm2_5': 8.36, 'pm10': 9.88, 'nh3': 0.31}"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#air-quality-history",
    "href": "posts/Environmental Machine Learning Project/index.html#air-quality-history",
    "title": "Environmental Data Project",
    "section": "Air Quality History",
    "text": "Air Quality History\nThe above code is testing the usage of the API. I looked up the coordinates of Tokyo and plugged that in to obtain the Air Quality Index. I received a value of 2. Now let’s trying using the API to obtain the Air Quality History.\n\nimport requests\nimport pandas as pd\nfrom datetime import datetime\n\nAPI_KEY = \"dc622205c25e4f765124ba4c03f370ba\"\nLAT = 35.6764\nLON = 139.6500\n\nstart = int(datetime(2025, 1, 1).timestamp())\nend = int(datetime(2025, 1, 6).timestamp())  \n\nurl = \"http://api.openweathermap.org/data/2.5/air_pollution/history\"\nparams = {\n    \"lat\": LAT,\n    \"lon\": LON,\n    \"start\": start,\n    \"end\": end,\n    \"appid\": API_KEY\n}\n\nresponse = requests.get(url, params=params)\ndata = response.json()\n\nrecords = []\nfor entry in data.get(\"list\", []):\n    ts = datetime.fromtimestamp(entry[\"dt\"])\n    aqi = entry[\"main\"][\"aqi\"]\n    components = entry[\"components\"]\n    components[\"timestamp\"] = ts\n    components[\"aqi\"] = aqi\n    records.append(components)\n\ndf = pd.DataFrame(records)\ndf.set_index(\"timestamp\", inplace=True)\n\nprint(df)\n\n                         co      no     no2     o3    so2  pm2_5   pm10   nh3  \\\ntimestamp                                                                       \n2025-01-01 00:00:00  333.79    0.00   26.73  57.22  19.07   0.86   2.03  1.03   \n2025-01-01 01:00:00  323.77    0.00   23.65  59.37  16.45   0.72   1.90  0.95   \n2025-01-01 02:00:00  317.10    0.00   22.62  60.80  15.74   0.65   1.91  0.90   \n2025-01-01 03:00:00  317.10    0.00   23.31  61.51  16.21   0.67   2.04  0.89   \n2025-01-01 04:00:00  323.77    0.00   24.68  60.80  17.40   0.72   2.18  0.93   \n...                     ...     ...     ...    ...    ...    ...    ...   ...   \n2025-01-05 20:00:00  834.47  236.03  126.12   0.00  72.48  22.26  31.77  3.96   \n2025-01-05 21:00:00  741.00  189.54  116.53   0.00  58.65  20.81  28.91  3.64   \n2025-01-05 22:00:00  554.08   98.35   98.71   0.00  47.21  16.70  22.60  2.94   \n2025-01-05 23:00:00  500.68   75.10   89.11   0.00  40.53  16.16  21.01  2.28   \n2025-01-06 00:00:00  480.65   64.37   84.31   0.00  31.47  16.41  20.80  1.63   \n\n                     aqi  \ntimestamp                 \n2025-01-01 00:00:00    1  \n2025-01-01 01:00:00    1  \n2025-01-01 02:00:00    2  \n2025-01-01 03:00:00    2  \n2025-01-01 04:00:00    2  \n...                  ...  \n2025-01-05 20:00:00    3  \n2025-01-05 21:00:00    3  \n2025-01-05 22:00:00    3  \n2025-01-05 23:00:00    3  \n2025-01-06 00:00:00    3  \n\n[121 rows x 9 columns]\n\n\nNow that we have our history of the air quality over the past five days at the start of 2025, we can try to categorize it based on https://www.iaqdetectors.com/blogs/what-is-air-quality-index-uba-standardhttps://www.iaqdetectors.com/blogs/what-is-air-quality-index-uba-standard, which says: Level 1: Excellent Air Quality. AQI Range: 1 (Green) Level 2: Good Air Quality. AQI Range: 2 (Green) Level 3: Moderate Air Quality. AQI Range: 3 (Yellow) Level 4: Poor Air Quality. AQI Range: 4 (Orange) Level 5: Very Poor Air Quality. AQI Range: 5 (Red or Purple)"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#categorizing-air-quality",
    "href": "posts/Environmental Machine Learning Project/index.html#categorizing-air-quality",
    "title": "Environmental Data Project",
    "section": "Categorizing Air Quality",
    "text": "Categorizing Air Quality\n\ndef categorize_aqi(aqi):\n    if aqi == 1:\n        return \"Excellent\"\n    elif aqi == 2:\n        return \"Good\"\n    elif aqi == 3:\n        return \"Moderate\"\n    elif aqi == 4:\n        return \"Poor\"\n    else:\n        return \"Very Poor\"\n\ndf[\"risk_level\"] = df[\"aqi\"].apply(categorize_aqi)\n\nprint(df)\n\n                         co      no     no2     o3    so2  pm2_5   pm10   nh3  \\\ntimestamp                                                                       \n2025-01-01 00:00:00  333.79    0.00   26.73  57.22  19.07   0.86   2.03  1.03   \n2025-01-01 01:00:00  323.77    0.00   23.65  59.37  16.45   0.72   1.90  0.95   \n2025-01-01 02:00:00  317.10    0.00   22.62  60.80  15.74   0.65   1.91  0.90   \n2025-01-01 03:00:00  317.10    0.00   23.31  61.51  16.21   0.67   2.04  0.89   \n2025-01-01 04:00:00  323.77    0.00   24.68  60.80  17.40   0.72   2.18  0.93   \n...                     ...     ...     ...    ...    ...    ...    ...   ...   \n2025-01-05 20:00:00  834.47  236.03  126.12   0.00  72.48  22.26  31.77  3.96   \n2025-01-05 21:00:00  741.00  189.54  116.53   0.00  58.65  20.81  28.91  3.64   \n2025-01-05 22:00:00  554.08   98.35   98.71   0.00  47.21  16.70  22.60  2.94   \n2025-01-05 23:00:00  500.68   75.10   89.11   0.00  40.53  16.16  21.01  2.28   \n2025-01-06 00:00:00  480.65   64.37   84.31   0.00  31.47  16.41  20.80  1.63   \n\n                     aqi risk_level  \ntimestamp                            \n2025-01-01 00:00:00    1  Excellent  \n2025-01-01 01:00:00    1  Excellent  \n2025-01-01 02:00:00    2       Good  \n2025-01-01 03:00:00    2       Good  \n2025-01-01 04:00:00    2       Good  \n...                  ...        ...  \n2025-01-05 20:00:00    3   Moderate  \n2025-01-05 21:00:00    3   Moderate  \n2025-01-05 22:00:00    3   Moderate  \n2025-01-05 23:00:00    3   Moderate  \n2025-01-06 00:00:00    3   Moderate  \n\n[121 rows x 10 columns]"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#line-graph-on-air-quality",
    "href": "posts/Environmental Machine Learning Project/index.html#line-graph-on-air-quality",
    "title": "Environmental Data Project",
    "section": "Line graph on Air Quality",
    "text": "Line graph on Air Quality\nAfter, we can visualize the data over the 5 day period!\n\ndef AQI_Plot(df, start, end, location):\n    fig = px.line(df, x=df.index, y=\"aqi\", title=f\"Air Quality Index (AQI) in {location} from {start} to {end}\",\n              labels={\"AQI\": \"AQI\", \"timestamp\": \"Date\"})\n\n    fig.update_traces(line=dict(color='blue'))\n    fig.update_layout(yaxis_title=\"AQI\", xaxis_title=\"Date\")\n\n    fig.show()\nAQI_Plot(df, \"2025-01-01\" ,\"2025-01-06\", \"Tokyo\" )\n\n\n\n\nHowever, if we wanted to incorporate the risk levels onto the visualization, we can add some color identification by specifying the color discrete map. The downside is that it is no longer continuous and a little harder to understand since the levels change so dramatically.\n\nimport plotly.express as px\n\ndef AQI_Color_Plot(df, start, end, location):\n\n    fig = px.scatter(df, x=df.index, y=\"aqi\", color=\"risk_level\",\n                    title=f\"Air Quality Index (AQI) in {location} from {start} to {end}\",\n                    labels={\"aqi\": \"AQI\", \"timestamp\": \"Date\"},\n                    color_discrete_map={\n                        \"Excellent\": \"green\",\n                        \"Good\": \"limegreen\",\n                        \"Moderate\": \"orange\",\n                        \"Poor\": \"orangered\",\n                        \"Very Poor\": \"red\"\n                    })\n\n    fig.update_traces(mode=\"lines+markers\")\n    fig.update_layout(yaxis_title=\"AQI\", xaxis_title=\"Date\")\n\n    fig.show()\nAQI_Color_Plot(df, \"2025-01-01\" ,\"2025-01-06\", \"Tokyo\" )\n\n\n\n\nNow let’s wrap everything into a function so that the parameters can be easily changed based on user input.\n\ndef create_df(lat, lon, start, end, location):\n    API_KEY = \"dc622205c25e4f765124ba4c03f370ba\"\n    LAT = lat\n    LON = lon\n\n    start = int(datetime.strptime(start, \"%Y-%m-%d\").timestamp())\n    end = int(datetime.strptime(end, \"%Y-%m-%d\").timestamp())\n\n    url = \"http://api.openweathermap.org/data/2.5/air_pollution/history\"\n    params = {\n        \"lat\": LAT,\n        \"lon\": LON,\n        \"start\": start,\n        \"end\": end,\n        \"appid\": API_KEY\n    }\n\n    response = requests.get(url, params=params)\n    data = response.json()\n\n    records = []\n    for entry in data.get(\"list\", []):\n        ts = datetime.fromtimestamp(entry[\"dt\"])\n        aqi = entry[\"main\"][\"aqi\"]\n        components = entry[\"components\"]\n        components[\"timestamp\"] = ts\n        components[\"aqi\"] = aqi\n        records.append(components)\n\n        df = pd.DataFrame(records)\n        df.set_index(\"timestamp\", inplace=True)\n        \n        def categorize_aqi(aqi):\n            if aqi == 1:\n                return \"Excellent\"\n            elif aqi == 2:\n                return \"Good\"\n            elif aqi == 3:\n                return \"Moderate\"\n            elif aqi == 4:\n                return \"Poor\"\n            else:\n                return \"Very Poor\"\n\n    df[\"risk_level\"] = df[\"aqi\"].apply(categorize_aqi)\n\n    print(df)\n    AQI_Color_Plot(df, start, end, location )"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#example-usage-of-our-function-for-the-specific-location-los-angeles",
    "href": "posts/Environmental Machine Learning Project/index.html#example-usage-of-our-function-for-the-specific-location-los-angeles",
    "title": "Environmental Data Project",
    "section": "Example usage of our function for the specific location Los Angeles",
    "text": "Example usage of our function for the specific location Los Angeles\nHere we are using the coordinates of Los Angeles and observing the air quality between Jan. 5 and Jan. 10, 2025, which was during the wildfires in the Palisades and Eaton Canyon.\n\nlat = 34.0549\nlon = 118.2426\nstart_date = \"2025-01-05\" \nend_date = \"2025-01-10\" \nlocation = \"Los Angeles\"\n\ndf = create_df(lat, lon, start_date, end_date, location)\nprint(df)\n\n                          co     no    no2     o3    so2   pm2_5    pm10  \\\ntimestamp                                                                  \n2025-01-05 00:00:00  1735.69  29.95  58.95   0.00  15.74  173.36  225.51   \n2025-01-05 01:00:00  1642.23  29.50  61.01   0.00  14.78  173.32  217.93   \n2025-01-05 02:00:00  1562.12  29.50  63.75   0.00  14.90  175.63  215.06   \n2025-01-05 03:00:00  1482.01  28.61  67.17   0.00  15.50  178.79  215.36   \n2025-01-05 04:00:00  1415.25  25.70  70.60   0.00  16.45  187.26  221.11   \n...                      ...    ...    ...    ...    ...     ...     ...   \n2025-01-09 20:00:00   614.17   0.02  43.87  33.97  42.92   38.82   61.52   \n2025-01-09 21:00:00   620.84   0.02  44.55  32.54  44.82   38.22   61.39   \n2025-01-09 22:00:00   647.54   0.04  49.35  28.25  51.98   39.03   61.56   \n2025-01-09 23:00:00   754.36   0.14  61.69  17.70  62.94   47.53   71.75   \n2025-01-10 00:00:00   834.47   0.31  71.29  11.00  66.76   54.43   79.63   \n\n                      nh3  aqi risk_level  \ntimestamp                                  \n2025-01-05 00:00:00  3.20    5  Very Poor  \n2025-01-05 01:00:00  2.03    5  Very Poor  \n2025-01-05 02:00:00  1.33    5  Very Poor  \n2025-01-05 03:00:00  0.81    5  Very Poor  \n2025-01-05 04:00:00  0.34    5  Very Poor  \n...                   ...  ...        ...  \n2025-01-09 20:00:00  6.46    3   Moderate  \n2025-01-09 21:00:00  7.16    3   Moderate  \n2025-01-09 22:00:00  7.41    3   Moderate  \n2025-01-09 23:00:00  8.61    3   Moderate  \n2025-01-10 00:00:00  9.88    4       Poor  \n\n[121 rows x 10 columns]\n\n\n\n\n\nNone"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#normalization-techniques-to-account-for-differences-in-country-size",
    "href": "posts/Environmental Machine Learning Project/index.html#normalization-techniques-to-account-for-differences-in-country-size",
    "title": "Environmental Data Project",
    "section": "Normalization techniques to account for differences in country size",
    "text": "Normalization techniques to account for differences in country size\n“This is likely due to their large size, which means that there is more surface area taken into account.” -&gt; Would it be possible to apply some normalization techniques to account for differences in country size? -&gt; answer: Great idea! We could try to find another data source that contains the country size and use data techniques to merge it together. Once we’ve prepared the data, we can perform normalization techniques by dividing the number of natural disasters with the corresponding country’s size."
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#obtain-world-population-dataset",
    "href": "posts/Environmental Machine Learning Project/index.html#obtain-world-population-dataset",
    "title": "Environmental Data Project",
    "section": "Obtain World Population Dataset",
    "text": "Obtain World Population Dataset\nFirst thing, we want a dataset on the population of each country. By doing a Google Search, I was able to find one on Kaggle.\n\n\n\n\ndf_pop = pd.read_csv('world_population.csv')\ndf_pop\n\n\n\n\n\n\n\n\nRank\nCCA3\nCountry/Territory\nCapital\nContinent\n2022 Population\n2020 Population\n2015 Population\n2010 Population\n2000 Population\n1990 Population\n1980 Population\n1970 Population\nArea (km²)\nDensity (per km²)\nGrowth Rate\nWorld Population Percentage\n\n\n\n\n0\n36\nAFG\nAfghanistan\nKabul\nAsia\n41128771\n38972230\n33753499\n28189672\n19542982\n10694796\n12486631\n10752971\n652230\n63.0587\n1.0257\n0.52\n\n\n1\n138\nALB\nAlbania\nTirana\nEurope\n2842321\n2866849\n2882481\n2913399\n3182021\n3295066\n2941651\n2324731\n28748\n98.8702\n0.9957\n0.04\n\n\n2\n34\nDZA\nAlgeria\nAlgiers\nAfrica\n44903225\n43451666\n39543154\n35856344\n30774621\n25518074\n18739378\n13795915\n2381741\n18.8531\n1.0164\n0.56\n\n\n3\n213\nASM\nAmerican Samoa\nPago Pago\nOceania\n44273\n46189\n51368\n54849\n58230\n47818\n32886\n27075\n199\n222.4774\n0.9831\n0.00\n\n\n4\n203\nAND\nAndorra\nAndorra la Vella\nEurope\n79824\n77700\n71746\n71519\n66097\n53569\n35611\n19860\n468\n170.5641\n1.0100\n0.00\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n229\n226\nWLF\nWallis and Futuna\nMata-Utu\nOceania\n11572\n11655\n12182\n13142\n14723\n13454\n11315\n9377\n142\n81.4930\n0.9953\n0.00\n\n\n230\n172\nESH\nWestern Sahara\nEl Aaiún\nAfrica\n575986\n556048\n491824\n413296\n270375\n178529\n116775\n76371\n266000\n2.1654\n1.0184\n0.01\n\n\n231\n46\nYEM\nYemen\nSanaa\nAsia\n33696614\n32284046\n28516545\n24743946\n18628700\n13375121\n9204938\n6843607\n527968\n63.8232\n1.0217\n0.42\n\n\n232\n63\nZMB\nZambia\nLusaka\nAfrica\n20017675\n18927715\n16248230\n13792086\n9891136\n7686401\n5720438\n4281671\n752612\n26.5976\n1.0280\n0.25\n\n\n233\n74\nZWE\nZimbabwe\nHarare\nAfrica\n16320537\n15669666\n14154937\n12839771\n11834676\n10113893\n7049926\n5202918\n390757\n41.7665\n1.0204\n0.20\n\n\n\n\n234 rows × 17 columns"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#create-and-use-sql-databases-for-data-manipulation",
    "href": "posts/Environmental Machine Learning Project/index.html#create-and-use-sql-databases-for-data-manipulation",
    "title": "Environmental Data Project",
    "section": "Create and Use SQL Databases for Data Manipulation",
    "text": "Create and Use SQL Databases for Data Manipulation\nSo now, we have two seperate databases. One on EMDAT data and the other one containing information of different countries. Because our goal is to apply some normalization techniques to account for differences in country size, we want to use SQL to merge the two databases. Our first step is to import SQL and then start a connection. With this connection, we can convert the two df’s into sql.\n\nimport sqlite3\nconn = sqlite3.connect(\"pop.sqlite\")\n\n\n\nwith sqlite3.connect(\"pop.sqlite\") as conn:\n    df_pop.to_sql(\"pop\", conn, if_exists=\"replace\", index=False)\n    map_df.to_sql(\"emdat\", conn, if_exists=\"replace\", index=False)\n\n\nwith sqlite3.connect(\"pop.sqlite\") as conn:\n    merged_df = pd.read_sql_query(\"\"\"\n        SELECT \n            e.*,\n            p.\"Area (km²)\"\n        FROM \n            emdat e\n        JOIN \n            pop p\n        ON \n            LOWER(e.Country) = LOWER(p.\"Country/Territory\")\n    \"\"\", conn)\n\nmerged_df\n\n\n\n\n\n\n\n\nHistoric\nClassification Key\nDisaster Group\nDisaster Subgroup\nDisaster Type\nDisaster Subtype\nISO\nCountry\nSubregion\nRegion\n...\nEnd Day\nTotal Deaths\nNo. Injured\nNo. Affected\nNo. Homeless\nTotal Affected\nTotal Damage ('000 US$)\nTotal Damage, Adjusted ('000 US$)\nCPI\nArea (km²)\n\n\n\n\n0\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nCHN\nChina\nEastern Asia\nAsia\n...\n14.0\n7.0\n2528.0\n1760000.0\n92479.0\n1855007.0\n73500.0\n130056.0\n56.514291\n9706961\n\n\n1\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nCHN\nChina\nEastern Asia\nAsia\n...\n26.0\n1.0\n2.0\n10300.0\nNaN\n10302.0\n483.0\n855.0\n56.514291\n9706961\n\n\n2\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nCOL\nColombia\nLatin America and the Caribbean\nAmericas\n...\n8.0\n2.0\nNaN\n430.0\nNaN\n430.0\nNaN\nNaN\n56.514291\n1141748\n\n\n3\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nIDN\nIndonesia\nSouth-eastern Asia\nAsia\n...\n4.0\n45.0\n270.0\nNaN\n52500.0\n52770.0\n30000.0\n53084.0\n56.514291\n1904569\n\n\n4\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nIDN\nIndonesia\nSouth-eastern Asia\nAsia\n...\n4.0\n103.0\n2714.0\n200000.0\n2000.0\n204714.0\n41000.0\n72548.0\n56.514291\n1904569\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1127\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nCHN\nChina\nEastern Asia\nAsia\n...\n7.0\n126.0\n188.0\n46500.0\nNaN\n46688.0\nNaN\nNaN\nNaN\n9706961\n\n\n1128\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nETH\nEthiopia\nSub-Saharan Africa\nAfrica\n...\n11.0\n2.0\nNaN\n99000.0\nNaN\n99000.0\nNaN\nNaN\nNaN\n1104300\n\n\n1129\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nMMR\nMyanmar\nSouth-eastern Asia\nAsia\n...\n28.0\n3784.0\n4824.0\n282790.0\nNaN\n287614.0\nNaN\nNaN\nNaN\n676578\n\n\n1130\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nTHA\nThailand\nSouth-eastern Asia\nAsia\n...\n28.0\n44.0\n37.0\n2313.0\nNaN\n2350.0\nNaN\nNaN\nNaN\n513120\n\n\n1131\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nTJK\nTajikistan\nCentral Asia\nAsia\n...\n13.0\n1.0\nNaN\n145.0\nNaN\n145.0\nNaN\nNaN\nNaN\n143100\n\n\n\n\n1132 rows × 30 columns"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#visualization",
    "href": "posts/Environmental Machine Learning Project/index.html#visualization",
    "title": "Environmental Data Project",
    "section": "Visualization",
    "text": "Visualization\nNow we can create the new visualization, but this time, the deaths are normalized by the country size.\n\ndisaster_counts = merged_df.groupby('Country').size().reset_index(name='disaster_count')\n\nmerged_df = merged_df.merge(disaster_counts, on='Country', how='left')\n\nmerged_df['disasters_normalized_by_size'] = round(\n    merged_df['disaster_count'] / merged_df['Area (km²)'] * 100000, 2\n)\n\n\n\nmerged_df\n\n\n\n\n\n\n\n\nHistoric\nClassification Key\nDisaster Group\nDisaster Subgroup\nDisaster Type\nDisaster Subtype\nISO\nCountry\nSubregion\nRegion\n...\nNo. Injured\nNo. Affected\nNo. Homeless\nTotal Affected\nTotal Damage ('000 US$)\nTotal Damage, Adjusted ('000 US$)\nCPI\nArea (km²)\ndisaster_count\ndisasters_normalized_by_size\n\n\n\n\n0\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nCHN\nChina\nEastern Asia\nAsia\n...\n2528.0\n1760000.0\n92479.0\n1855007.0\n73500.0\n130056.0\n56.514291\n9706961\n114\n1.17\n\n\n1\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nCHN\nChina\nEastern Asia\nAsia\n...\n2.0\n10300.0\nNaN\n10302.0\n483.0\n855.0\n56.514291\n9706961\n114\n1.17\n\n\n2\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nCOL\nColombia\nLatin America and the Caribbean\nAmericas\n...\nNaN\n430.0\nNaN\n430.0\nNaN\nNaN\n56.514291\n1141748\n18\n1.58\n\n\n3\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nIDN\nIndonesia\nSouth-eastern Asia\nAsia\n...\n270.0\nNaN\n52500.0\n52770.0\n30000.0\n53084.0\n56.514291\n1904569\n101\n5.30\n\n\n4\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nIDN\nIndonesia\nSouth-eastern Asia\nAsia\n...\n2714.0\n200000.0\n2000.0\n204714.0\n41000.0\n72548.0\n56.514291\n1904569\n101\n5.30\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1127\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nCHN\nChina\nEastern Asia\nAsia\n...\n188.0\n46500.0\nNaN\n46688.0\nNaN\nNaN\nNaN\n9706961\n114\n1.17\n\n\n1128\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nETH\nEthiopia\nSub-Saharan Africa\nAfrica\n...\nNaN\n99000.0\nNaN\n99000.0\nNaN\nNaN\nNaN\n1104300\n10\n0.91\n\n\n1129\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nMMR\nMyanmar\nSouth-eastern Asia\nAsia\n...\n4824.0\n282790.0\nNaN\n287614.0\nNaN\nNaN\nNaN\n676578\n13\n1.92\n\n\n1130\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nTHA\nThailand\nSouth-eastern Asia\nAsia\n...\n37.0\n2313.0\nNaN\n2350.0\nNaN\nNaN\nNaN\n513120\n21\n4.09\n\n\n1131\nNo\nnat-geo-ear-gro\nNatural\nGeophysical\nEarthquake\nGround movement\nTJK\nTajikistan\nCentral Asia\nAsia\n...\nNaN\n145.0\nNaN\n145.0\nNaN\nNaN\nNaN\n143100\n9\n6.29\n\n\n\n\n1132 rows × 32 columns\n\n\n\n\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nfig = px.scatter(\n    merged_df.query('disaster_count'),\n    x=\"Country\",\n    y=\"disaster_count\",\n    size=\"disasters_normalized_by_size\",\n    color=\"Country\",\n    hover_name=\"Country\",\n    log_y=True, \n    size_max=100,\n    title=\"Number of Disaster Types by Country\"\n)\n\nfig.show()"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#lets-revist-our-first-research-question",
    "href": "posts/Environmental Machine Learning Project/index.html#lets-revist-our-first-research-question",
    "title": "Environmental Data Project",
    "section": "Let’s revist our first research question",
    "text": "Let’s revist our first research question\nNow that we normalized the disaster count by size of the country, we can take a second look at the research question. 1. “Which countries have experienced the highest number of natural disasters?” - We’ll need to investigate variables on countries, when it occured, and type of disaster"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#analysis-of-first-research-question-1",
    "href": "posts/Environmental Machine Learning Project/index.html#analysis-of-first-research-question-1",
    "title": "Environmental Data Project",
    "section": "Analysis of first research question",
    "text": "Analysis of first research question\nFrom this visualization, we can see that Rwanda, El Savador, and the Philippines have experienced the highest number of natural disasters per 100,000 squared km. Comparing this with the original visualization, we can see that there are differences in the coutnries with the highest numebr of natural diasters where we previously found India, China, and the USA noteworthy due to their large size."
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#download-machine-learning-imports",
    "href": "posts/Environmental Machine Learning Project/index.html#download-machine-learning-imports",
    "title": "Environmental Data Project",
    "section": "Download machine learning imports",
    "text": "Download machine learning imports\n\nimport numpy as np\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nimport keras\nimport tensorflow as tf\nkeras.__version__\n\n'3.10.0'"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#choose-dataset-for-machine-learning",
    "href": "posts/Environmental Machine Learning Project/index.html#choose-dataset-for-machine-learning",
    "title": "Environmental Data Project",
    "section": "Choose dataset for machine learning",
    "text": "Choose dataset for machine learning\nOur dataset is multi-spectral values of pixels in 3x3 neighbourhoods in a satellite image, and the classification associated with the central pixel in each neighbourhood. We are using machine learning and 36 data variables to predict the seven types of classes which are:\n1 red soil 2 cotton crop 3 grey soil 4 damp grey soil 5 soil with vegetation stubble 6 mixture class (all types present) 7 very damp grey soil\nThe link to the dataset is: https://archive.ics.uci.edu/dataset/146/statlog+landsat+satellite.\n\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.trn\"\ncol_names = [f\"feature_{i}\" for i in range(36)] + [\"label\"]\ndf = pd.read_csv(url, sep='\\\\s+', header=None, names=col_names)\n\n# Drop na\ndf = df.dropna()\n\nX = df.drop(\"label\", axis=1).astype(np.float32).values #predictor data is everything except label\ny = df[\"label\"].values #target data is the label\n\nle = LabelEncoder()\ny = le.fit_transform(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#investigate-data",
    "href": "posts/Environmental Machine Learning Project/index.html#investigate-data",
    "title": "Environmental Data Project",
    "section": "Investigate data",
    "text": "Investigate data\nLet’s look at the shape, as well as the first five rows of the predictor data and the target data.\n\nX_train.shape\n\n(3104, 36)\n\n\n\nX_train[:5] # first five rows of predictor data\n\narray([[ 63.,  70.,  72.,  57.,  67.,  77.,  72.,  60.,  71.,  77.,  72.,\n         64.,  70.,  75.,  82.,  62.,  70.,  79.,  82.,  65.,  70.,  79.,\n         85.,  65.,  68.,  77.,  74.,  61.,  68.,  77.,  78.,  61.,  72.,\n         81.,  82.,  65.],\n       [ 63.,  67.,  69.,  55.,  63.,  71.,  69.,  55.,  63.,  67.,  73.,\n         55.,  67.,  66.,  72.,  53.,  63.,  70.,  68.,  53.,  67.,  70.,\n         72.,  57.,  63.,  67.,  70.,  55.,  63.,  71.,  74.,  55.,  63.,\n         67.,  74.,  55.],\n       [ 75.,  83.,  85.,  71.,  71.,  75.,  85.,  67.,  71.,  79.,  77.,\n         67.,  82.,  96., 100.,  81.,  78.,  83.,  84.,  70.,  74.,  75.,\n         88.,  66.,  87., 103., 105.,  83.,  79.,  88.,  97.,  72.,  71.,\n         81.,  86.,  68.],\n       [ 88., 106., 115.,  87.,  88., 111., 111.,  91.,  88., 106., 115.,\n         87.,  88., 107., 118.,  92.,  88., 112., 113.,  88.,  88., 103.,\n        113.,  88.,  88., 107., 113.,  87.,  88., 107., 104.,  87.,  88.,\n        107., 109.,  83.],\n       [ 68.,  69.,  86.,  76.,  68.,  69.,  86.,  72.,  68.,  73.,  86.,\n         72.,  64.,  71.,  87.,  74.,  64.,  71.,  87.,  78.,  68.,  71.,\n         87.,  74.,  67.,  68.,  89.,  79.,  67.,  68.,  89.,  75.,  67.,\n         72.,  85.,  71.]], dtype=float32)\n\n\n\ny_train[:5] # first five rows of target data\n\narray([5, 5, 5, 2, 4])\n\n\n\nfrom keras import layers\n\nmodel = keras.models.Sequential([\n    layers.Input((36,)), \n    layers.Dense(500, activation=\"relu\"),\n    layers.Dense(500, activation=\"relu\"),\n    layers.Dense(len(np.unique(y))) \n])\n\n\nmodel.summary()\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (Dense)                   │ (None, 500)            │        18,500 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 500)            │       250,500 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (Dense)                 │ (None, 6)              │         3,006 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 272,006 (1.04 MB)\n\n\n\n Trainable params: 272,006 (1.04 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#train-the-data-and-make-predictions",
    "href": "posts/Environmental Machine Learning Project/index.html#train-the-data-and-make-predictions",
    "title": "Environmental Data Project",
    "section": "Train the data and make predictions",
    "text": "Train the data and make predictions\n\nmodel(X_train[:5])\n\n&lt;tf.Tensor: shape=(5, 6), dtype=float32, numpy=\narray([[ 14.330679 ,  -4.664956 ,  -8.741774 ,  -6.880457 ,   1.873765 ,\n         14.979021 ],\n       [ 13.455207 ,  -4.072483 ,  -7.892398 ,  -7.3552794,   1.8109777,\n         13.604643 ],\n       [ 15.390393 ,  -5.3483734, -14.239582 ,  -7.51396  ,   3.959106 ,\n         19.957952 ],\n       [ 17.20682  ,  -5.180315 , -15.053029 ,  -9.738934 ,   3.3238802,\n         21.687113 ],\n       [ 11.1366415,  -3.3746386, -10.642702 ,  -7.3522043,   4.175025 ,\n         16.877045 ]], dtype=float32)&gt;\n\n\nWe can apply the softmax layer to transform the data into probabilities.\n\nsoftmax = keras.layers.Softmax()\nsoftmax(model(X_train[:5]))\n\n&lt;tf.Tensor: shape=(5, 6), dtype=float32, numpy=\narray([[3.43362778e-01, 1.93220573e-09, 3.27728643e-11, 2.10801640e-10,\n        1.33593164e-06, 6.56635940e-01],\n       [4.62708503e-01, 1.13013012e-08, 2.47834031e-10, 4.24060592e-10,\n        4.05772926e-06, 5.37287474e-01],\n       [1.02765663e-02, 1.01184998e-11, 1.39223979e-15, 1.16041627e-12,\n        1.11507283e-07, 9.89723325e-01],\n       [1.12031568e-02, 2.12193175e-12, 1.09412205e-16, 2.22304627e-14,\n        1.04726574e-08, 9.88796830e-01],\n       [3.20316898e-03, 1.59739166e-09, 1.11411770e-12, 2.99210275e-11,\n        3.03520619e-06, 9.96793687e-01]], dtype=float32)&gt;"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#compile-the-model-by-specifying-the-loss-function-and-optimization-algorithm",
    "href": "posts/Environmental Machine Learning Project/index.html#compile-the-model-by-specifying-the-loss-function-and-optimization-algorithm",
    "title": "Environmental Data Project",
    "section": "Compile the model, by specifying the loss function and optimization algorithm",
    "text": "Compile the model, by specifying the loss function and optimization algorithm\n\nloss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmodel.compile(optimizer= \"adam\", loss = loss_fn, metrics=[\"accuracy\"])\n\n\nhistory = model.fit(X_train, y_train, epochs = 20,  verbose=1)\n\nEpoch 1/20\n97/97 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.3960 - loss: 18.9771\nEpoch 2/20\n97/97 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.7031 - loss: 0.8707\nEpoch 3/20\n97/97 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.7096 - loss: 0.8774\nEpoch 4/20\n97/97 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.7150 - loss: 0.8075\nEpoch 5/20\n97/97 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.7459 - loss: 0.8138\nEpoch 6/20\n97/97 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.7558 - loss: 0.6525\nEpoch 7/20\n97/97 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.7633 - loss: 0.6328\nEpoch 8/20\n97/97 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.7969 - loss: 0.5224\nEpoch 9/20\n97/97 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.7761 - loss: 0.5531\nEpoch 10/20\n97/97 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.7913 - loss: 0.5354\nEpoch 11/20\n97/97 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.7813 - loss: 0.5683\nEpoch 12/20\n97/97 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.7826 - loss: 0.5241\nEpoch 13/20\n97/97 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.7893 - loss: 0.5272\nEpoch 14/20\n97/97 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.8069 - loss: 0.4950\nEpoch 15/20\n97/97 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.8215 - loss: 0.4513\nEpoch 16/20\n97/97 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.8076 - loss: 0.4766\nEpoch 17/20\n97/97 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.8012 - loss: 0.4842\nEpoch 18/20\n97/97 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.8338 - loss: 0.4325\nEpoch 19/20\n97/97 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.8124 - loss: 0.4657\nEpoch 20/20\n97/97 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.8049 - loss: 0.4804"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#plot-the-progress-of-the-training-over-time",
    "href": "posts/Environmental Machine Learning Project/index.html#plot-the-progress-of-the-training-over-time",
    "title": "Environmental Data Project",
    "section": "Plot the progress of the training over time",
    "text": "Plot the progress of the training over time\nThe training accuracy is improving over time and is hovering at around 80%.\n\nfrom matplotlib import pyplot as plt\nplt.plot(history.history[\"accuracy\"])\nplt.gca().set(xlabel=\"epoch\", ylabel=\"training accuracy\")"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#evaluate-the-model-on-our-test-data",
    "href": "posts/Environmental Machine Learning Project/index.html#evaluate-the-model-on-our-test-data",
    "title": "Environmental Data Project",
    "section": "Evaluate the model on our test data",
    "text": "Evaluate the model on our test data\nWe’re able to achieve an accuracy of around 80%.\n\nmodel.evaluate(X_test, y_test, verbose=2)\n\n42/42 - 0s - 3ms/step - accuracy: 0.7949 - loss: 0.5223\n\n\n[0.522253692150116, 0.7948910593986511]"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#prediction-probabilities",
    "href": "posts/Environmental Machine Learning Project/index.html#prediction-probabilities",
    "title": "Environmental Data Project",
    "section": "Prediction Probabilities",
    "text": "Prediction Probabilities\n\nmodel(X_train[:5])\n\n&lt;tf.Tensor: shape=(5, 6), dtype=float32, numpy=\narray([[-3.2564087 , -5.2264066 ,  0.77950776,  1.7083402 , -3.5599189 ,\n         2.9703608 ],\n       [-5.4485703 , -5.5300303 , -1.8344328 ,  0.73341477, -0.48724002,\n         3.4024415 ],\n       [-3.7386181 , -3.9295647 , -0.08160114,  0.8814881 , -0.9732225 ,\n         1.5727332 ],\n       [-0.3460873 , -5.9270325 ,  3.452885  ,  1.1476189 , -5.334968  ,\n        -1.6315415 ],\n       [-3.0256038 , -2.085518  , -3.086216  , -1.4295872 ,  2.5746348 ,\n         0.0109342 ]], dtype=float32)&gt;\n\n\n\nprob_model = keras.models.Sequential([\n    model,\n    layers.Softmax()\n])\n\n\nprob_model(X_train[:5])\n\n&lt;tf.Tensor: shape=(5, 6), dtype=float32, numpy=\narray([[1.4127031e-03, 1.9701147e-04, 7.9951584e-02, 2.0240161e-01,\n        1.0428891e-03, 7.1499419e-01],\n       [1.3076646e-04, 1.2053643e-04, 4.8539578e-03, 6.3284606e-02,\n        1.8671295e-02, 9.1293889e-01],\n       [2.7732656e-03, 2.2912072e-03, 1.0745181e-01, 2.8149977e-01,\n        4.4054218e-02, 5.6192970e-01],\n       [1.9842867e-02, 7.4787808e-05, 8.8608837e-01, 8.8371612e-02,\n        1.3519508e-04, 5.4870476e-03],\n       [3.3249243e-03, 8.5124793e-03, 3.1293808e-03, 1.6402991e-02,\n        8.9936203e-01, 6.9268137e-02]], dtype=float32)&gt;\n\n\n\npredictions = prob_model.predict(X_test).argmax(axis = 1)\npredictions\n\n42/42 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \n\n\narray([0, 2, 2, ..., 2, 2, 0])\n\n\n\n[le.classes_[predictions[i]] for i in range(10)]\n\n[1, 3, 3, 1, 5, 3, 1, 7, 5, 2]"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#learning-go-as-a-coding-language",
    "href": "posts/Environmental Machine Learning Project/index.html#learning-go-as-a-coding-language",
    "title": "Environmental Data Project",
    "section": "Learning Go as a Coding Language",
    "text": "Learning Go as a Coding Language\n\nhigh-level general purpose programming language\nsimple syntax\ndesigned at Google\nbuilt-in support for concurrency -&gt; handling multiple tasks simultaneously\nused for building cloud-based applications, microservices, and network infrastructure\ncan be used to create command-line tools\nfast build times and tools for monitoring and error -&gt; good for DevOps and SRE roles"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#binary-search",
    "href": "posts/Environmental Machine Learning Project/index.html#binary-search",
    "title": "Environmental Data Project",
    "section": "Binary Search",
    "text": "Binary Search\nLet’s learn what binary search is and how it’s useful in supporting several machine learning tasks, like optimization and search.\nLet’s practice this problem using Go: https://leetcode.com/problems/binary-search/\nGiven an array of integers nums which is sorted in ascending order, and an integer target, write a function to search target in nums. If target exists, then return its index. Otherwise, return -1.\n\nfunc search(nums []int, target int) int {\n    l := 0\n    r := len(nums) - 1\n    var mid int\n    for l &lt;= r {\n        mid = (r + l) / 2\n        if nums[mid] &lt; target {\n            l = mid + 1\n        } else if nums[mid] &gt; target {\n            r = mid - 1\n        } else {\n            return mid\n        }\n    }\n    return -1\n}\n\nFor this problem, we are technique where we are dividing the list by half until we are able to reach the target number. We find the middle number using mid = (r + l) / 2, and then we compare whether this middle number is bigger or smaller than the target number. If it’s the same, we return the index."
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#decision-trees",
    "href": "posts/Environmental Machine Learning Project/index.html#decision-trees",
    "title": "Environmental Data Project",
    "section": "Decision Trees",
    "text": "Decision Trees\nDecision trees are a core component of many ML models, such as Random Forests (multiple decision trees to reduce variance) and Gradient Boosted Trees (ensemble of decision trees, where each tree is sequentially trained to correct the errors of the previous trees).\n\nTree Traversal\nTree traverals are important in decision trees, because the type of traversal can help make navigating the tree more efficient. For example, depth-first-search is best at finding the shortest path in an unweighted graph. However, breath-for-search is best at detecting cycles and performing topological sorting.\n\n\nDFS\nLet’s practice Binary Tree Inorder Traversal: https://leetcode.com/problems/binary-tree-inorder-traversal/description/\nGiven the root of a binary tree, return the inorder traversal of its nodes’ values.\n\n/**\n * Definition for a binary tree node.\n * type TreeNode struct {\n *     Val int\n *     Left *TreeNode\n *     Right *TreeNode\n * }\n */\n\nfunc inorderTraversal(root *TreeNode) []int {\n\n    ans := []int{}\n    var inorder func(*TreeNode)\n\n    inorder = func(root *TreeNode){\n        if root == nil{\n            return\n        }\n        inorder(root.Left)\n        ans = append(ans, root.Val)\n        inorder(root.Right)\n    }\n    inorder(root)\n\n    return ans;\n}\n\nTo perform inorder traversal, we use a recursion function to travel down the left side, then print the current value, and then travel down the right side. We keep performing this until we’ve hit every node.\n\n\nBFS\nNow let’s try Binary Tree Level Order Traversal: https://leetcode.com/problems/binary-tree-level-order-traversal/description/\nGiven the root of a binary tree, return the level order traversal of its nodes’ values. (i.e., from left to right, level by level).\n\n/**\n * Definition for a binary tree node.\n * type TreeNode struct {\n *     Val int\n *     Left *TreeNode\n *     Right *TreeNode\n * }\n */\n \nfunc levelOrder(root *TreeNode) [][]int {\n    if root == nil{\n        return [][] int{}\n    }\n        var ans [][] int\n        q :=[]*TreeNode{root}\n        for len(q) &gt; 0{\n            qlen:= len(q)\n            var temp []int\n            for i :=0; i &lt; qlen; i++ {\n                node := q[0]\n                temp = append(temp, node.Val)\n                q = q[1:]\n                \n                if node.Left != nil{\n                    q = append(q, node.Left)\n                }\n                \n                if node.Right != nil{\n                    q = append(q, node.Right)\n                }\n            }\n            ans = append(ans, temp)\n        }\n        return ans\n}\n\nTo perform BFS, we use the data structure called a queue. This is helpful in traveling through the nodes level by level. While there is something in a queue, we keep track of the length and then pop at the first index to access the node. With this node, we store the value and then add the left and right nodes to the queue. Then we add it to the final answer array."
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#binary-search-tree",
    "href": "posts/Environmental Machine Learning Project/index.html#binary-search-tree",
    "title": "Environmental Data Project",
    "section": "Binary Search Tree",
    "text": "Binary Search Tree\nBinary Search Trees are helpful for machine learning applications, specifically decision trees and for efficient data handling. They provide useful for lookups, insertions, and deletions when processing and optimizing large datasets.\nLet’s try this problem 700. Search in a Binary Search Tree: https://leetcode.com/problems/search-in-a-binary-search-tree/description/\nYou are given the root of a binary search tree (BST) and an integer val.\nFind the node in the BST that the node’s value equals val and return the subtree rooted with that node. If such a node does not exist, return null.\n\n/**\n * Definition for a binary tree node.\n * type TreeNode struct {\n *     Val int\n *     Left *TreeNode\n *     Right *TreeNode\n * }\n */\nfunc searchBST(root *TreeNode, val int) *TreeNode {\n    if root == nil{\n        return nil\n    }\n    if val == root.Val{\n        return root\n    } else if val &gt; root.Val{\n        return searchBST(root.Right, val)\n    }\n    return searchBST(root.Left, val)\n}\n\nFor this problem, we are able to use the concept of how values that are smaller than the node are on the left, whereas values that are bigger are on the right. We can keep recursively calling the function with either the left or right node, depending on whether it’s smaller or bigger."
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#dataset-1",
    "href": "posts/Environmental Machine Learning Project/index.html#dataset-1",
    "title": "Environmental Data Project",
    "section": "Dataset",
    "text": "Dataset\nWe’ll be using the UCI Machine Learning Dataset on Air Quality. https://archive.ics.uci.edu/dataset/360/air+quality\n“It contains the responses of a gas multisensor device deployed on the field in an Italian city. Hourly responses averages are recorded along with gas concentrations references from a certified analyzer.\nThe dataset contains 9358 instances of hourly averaged responses from an array of 5 metal oxide chemical sensors embedded in an Air Quality Chemical Multisensor Device. The device was located on the field in a significantly polluted area, at road level,within an Italian city. Data were recorded from March 2004 to February 2005 (one year)representing the longest freely available recordings of on field deployed air quality chemical sensor devices responses. Ground Truth hourly averaged concentrations for CO, Non Metanic Hydrocarbons, Benzene, Total Nitrogen Oxides (NOx) and Nitrogen Dioxide (NO2) and were provided by a co-located reference certified analyzer. Evidences of cross-sensitivities as well as both concept and sensor drifts are present as described in De Vito et al., Sens. And Act. B, Vol. 129,2,2008 (citation required) eventually affecting sensors concentration estimation capabilities. Missing values are tagged with -200 value. This dataset can be used exclusively for research purposes. Commercial purposes are fully excluded.”"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#add-imports",
    "href": "posts/Environmental Machine Learning Project/index.html#add-imports",
    "title": "Environmental Data Project",
    "section": "Add Imports",
    "text": "Add Imports\nWe’re using sklearn Random Forests model and also adding in imports for calculating accuracy.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#load-data-and-prepare-and-clean-it",
    "href": "posts/Environmental Machine Learning Project/index.html#load-data-and-prepare-and-clean-it",
    "title": "Environmental Data Project",
    "section": "Load Data and Prepare and Clean It",
    "text": "Load Data and Prepare and Clean It\nLet’s read in the dataset. First we’ll drop unnamed column names. Then we’ll convert the Date and Time columns into a single one that is formatted in a standardized way. Then we’ll drop NA columns for Datetime. For the missing values, we’ll try to fill them in using linear interpolation, but if it’s not possible then we’ll remove any rows.\n\ndf = pd.read_csv('AirQualityUCI.csv', sep=';', decimal=',', na_values=-200)\ndf = df.loc[:, ~df.columns.str.contains('^Unnamed')] \n\ndf['Datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], format='%d/%m/%Y %H.%M.%S', errors='coerce')\ndf.dropna(subset=['Datetime'], inplace=True)\ndf.set_index('Datetime', inplace=True)\ndf.drop(columns=['Date', 'Time'], inplace=True)\n\ndf = df.interpolate(method='linear').dropna()\n\n\ndf\n\n\n\n\n\n\n\n\nfeature_0\nfeature_1\nfeature_2\nfeature_3\nfeature_4\nfeature_5\nfeature_6\nfeature_7\nfeature_8\nfeature_9\n...\nfeature_27\nfeature_28\nfeature_29\nfeature_30\nfeature_31\nfeature_32\nfeature_33\nfeature_34\nfeature_35\nlabel\n\n\n\n\n0\n92\n115\n120\n94\n84\n102\n106\n79\n84\n102\n...\n104\n88\n121\n128\n100\n84\n107\n113\n87\n3\n\n\n1\n84\n102\n106\n79\n84\n102\n102\n83\n80\n102\n...\n100\n84\n107\n113\n87\n84\n99\n104\n79\n3\n\n\n2\n84\n102\n102\n83\n80\n102\n102\n79\n84\n94\n...\n87\n84\n99\n104\n79\n84\n99\n104\n79\n3\n\n\n3\n80\n102\n102\n79\n84\n94\n102\n79\n80\n94\n...\n79\n84\n99\n104\n79\n84\n103\n104\n79\n3\n\n\n4\n84\n94\n102\n79\n80\n94\n98\n76\n80\n102\n...\n79\n84\n103\n104\n79\n79\n107\n109\n87\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4430\n56\n64\n108\n96\n64\n71\n108\n96\n68\n75\n...\n92\n66\n83\n108\n96\n66\n87\n104\n89\n5\n\n\n4431\n64\n71\n108\n96\n68\n75\n108\n96\n71\n87\n...\n96\n66\n87\n104\n89\n63\n87\n104\n89\n5\n\n\n4432\n68\n75\n108\n96\n71\n87\n108\n88\n71\n91\n...\n89\n63\n87\n104\n89\n70\n100\n104\n85\n4\n\n\n4433\n71\n87\n108\n88\n71\n91\n100\n81\n76\n95\n...\n89\n70\n100\n104\n85\n70\n91\n104\n85\n4\n\n\n4434\n71\n91\n100\n81\n76\n95\n108\n88\n80\n95\n...\n85\n70\n91\n104\n85\n63\n91\n100\n81\n4\n\n\n\n\n4435 rows × 37 columns"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#create-lag-features",
    "href": "posts/Environmental Machine Learning Project/index.html#create-lag-features",
    "title": "Environmental Data Project",
    "section": "Create lag features",
    "text": "Create lag features\nLet’s select our predictor variable which is CO(GT). We want to investigate the data by breaking it down into little bits of data that get captured overtime. This will allow us to see how the data fluctuates to best predict future data. We’ll create a new column that capture the CO(GT) levels at every hour.\n\ntarget_col = 'CO(GT)'\nlag_hours = 24\n\nfor lag in range(1, lag_hours + 1):\n    df[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)\n\ndf.dropna(inplace=True)  \n\n\ndf\n\n\n\n\n\n\n\n\nCO(GT)\nPT08.S1(CO)\nNMHC(GT)\nC6H6(GT)\nPT08.S2(NMHC)\nNOx(GT)\nPT08.S3(NOx)\nNO2(GT)\nPT08.S4(NO2)\nPT08.S5(O3)\n...\nCO(GT)_lag_15\nCO(GT)_lag_16\nCO(GT)_lag_17\nCO(GT)_lag_18\nCO(GT)_lag_19\nCO(GT)_lag_20\nCO(GT)_lag_21\nCO(GT)_lag_22\nCO(GT)_lag_23\nCO(GT)_lag_24\n\n\nDatetime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2004-03-11 18:00:00\n4.8\n1581.0\n307.0\n20.8\n1319.0\n281.0\n799.0\n151.0\n2083.0\n1409.0\n...\n0.60\n0.90\n1.00\n1.20\n1.2\n1.6\n2.2\n2.2\n2.0\n2.6\n\n\n2004-03-11 19:00:00\n6.9\n1776.0\n461.0\n27.4\n1488.0\n383.0\n702.0\n172.0\n2333.0\n1704.0\n...\n0.65\n0.60\n0.90\n1.00\n1.2\n1.2\n1.6\n2.2\n2.2\n2.0\n\n\n2004-03-11 20:00:00\n6.1\n1640.0\n401.0\n24.0\n1404.0\n351.0\n743.0\n165.0\n2191.0\n1654.0\n...\n0.70\n0.65\n0.60\n0.90\n1.0\n1.2\n1.2\n1.6\n2.2\n2.2\n\n\n2004-03-11 21:00:00\n3.9\n1313.0\n197.0\n12.8\n1076.0\n240.0\n957.0\n136.0\n1707.0\n1285.0\n...\n0.70\n0.70\n0.65\n0.60\n0.9\n1.0\n1.2\n1.2\n1.6\n2.2\n\n\n2004-03-11 22:00:00\n1.5\n965.0\n61.0\n4.7\n749.0\n94.0\n1325.0\n85.0\n1333.0\n821.0\n...\n1.10\n0.70\n0.70\n0.65\n0.6\n0.9\n1.0\n1.2\n1.2\n1.6\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2005-04-04 10:00:00\n3.1\n1314.0\n275.0\n13.5\n1101.0\n472.0\n539.0\n190.0\n1374.0\n1729.0\n...\n2.70\n1.20\n1.40\n1.30\n1.1\n1.0\n1.0\n1.4\n1.3\n1.4\n\n\n2005-04-04 11:00:00\n2.4\n1163.0\n275.0\n11.4\n1027.0\n353.0\n604.0\n179.0\n1264.0\n1269.0\n...\n2.50\n2.70\n1.20\n1.40\n1.3\n1.1\n1.0\n1.0\n1.4\n1.3\n\n\n2005-04-04 12:00:00\n2.4\n1142.0\n275.0\n12.4\n1063.0\n293.0\n603.0\n175.0\n1241.0\n1092.0\n...\n1.50\n2.50\n2.70\n1.20\n1.4\n1.3\n1.1\n1.0\n1.0\n1.4\n\n\n2005-04-04 13:00:00\n2.1\n1003.0\n275.0\n9.5\n961.0\n235.0\n702.0\n156.0\n1041.0\n770.0\n...\n1.60\n1.50\n2.50\n2.70\n1.2\n1.4\n1.3\n1.1\n1.0\n1.0\n\n\n2005-04-04 14:00:00\n2.2\n1071.0\n275.0\n11.9\n1047.0\n265.0\n654.0\n168.0\n1129.0\n816.0\n...\n1.20\n1.60\n1.50\n2.50\n2.7\n1.2\n1.4\n1.3\n1.1\n1.0\n\n\n\n\n9333 rows × 37 columns"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#split-into-training-and-test-data",
    "href": "posts/Environmental Machine Learning Project/index.html#split-into-training-and-test-data",
    "title": "Environmental Data Project",
    "section": "Split into training and test data",
    "text": "Split into training and test data\nLet’s create a supervised machine learning model that does time series prediction. The input features will be the CO(GT) at each hour for the past day and the target variable is the actual CO(GT). We’ll split the data in such a way that 20% of it will be for testing purposes and the other 80% is for training.\n\nX = df[[f'{target_col}_lag_{i}' for i in range(1, lag_hours + 1)]]\ny = df[target_col]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#train-random-forest-model",
    "href": "posts/Environmental Machine Learning Project/index.html#train-random-forest-model",
    "title": "Environmental Data Project",
    "section": "Train random forest model",
    "text": "Train random forest model\nWe’re building 100 decision trees and setting a random seed.\n“A random forest is a meta estimator that fits a number of decision tree regressors on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. Trees in the forest use the best split strategy, i.e. equivalent to passing splitter=”best” to the underlying DecisionTreeRegressor. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.”\n\nmodel = RandomForestRegressor(n_estimators=100, random_state=50)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#evaluate",
    "href": "posts/Environmental Machine Learning Project/index.html#evaluate",
    "title": "Environmental Data Project",
    "section": "Evaluate",
    "text": "Evaluate\nLet’s figure out the coefficient of determination, the mean absoluate error, the root mean squared error, and the mean absoluate percentage error.\n\nr2 = r2_score(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nmape = mean_absolute_percentage_error(y_test, y_pred)\n\nprint(f\"R² Score: {r2:.4f}\")\nprint(f\"MAE: {mae:.4f}\")\nprint(f\"RMSE: {rmse:.4f}\")\nprint(f\"MAPE: {mape:.2%}\")\nprint(f\"Approximate Accuracy: {100 - mape * 100:.2f}%\")\n\nR² Score: 0.8152\nMAE: 0.3883\nRMSE: 0.5942\nMAPE: 29.66%\nApproximate Accuracy: 70.34%\n\n\nBased on the evaluation, the random forest model performs decently since it’s able to achieve 70% accuracy."
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#plot-accuracy-by-comparing-predictions-with-real-data",
    "href": "posts/Environmental Machine Learning Project/index.html#plot-accuracy-by-comparing-predictions-with-real-data",
    "title": "Environmental Data Project",
    "section": "Plot accuracy by comparing predictions with real data",
    "text": "Plot accuracy by comparing predictions with real data\nThe orange line represents the predicted data and the blue line represents the actual data.\n\nplt.figure(figsize=(12, 5))\nplt.plot(y_test.values, label='Actual CO(GT)')\nplt.plot(y_pred, label='Predicted CO(GT)')\nplt.title('Random Forest: CO(GT) Prediction')\nplt.xlabel('Time Step')\nplt.ylabel('CO (mg/m³)')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#importance-of-predicting-co",
    "href": "posts/Environmental Machine Learning Project/index.html#importance-of-predicting-co",
    "title": "Environmental Data Project",
    "section": "Importance of predicting CO",
    "text": "Importance of predicting CO\nThe data we are trying to predict is the true hourly averaged concentration CO in mg/m^3. It’s important to use machine learning to predict CO because carbon monoxide is a greenhouse gas that contributes to air pollution, which can cause acid rain, damage vegetation, and reduce visibility. By learning to accurately predict CO levels, we can develop better environmental protection measures to help counteract against the negative effects. It is crucial for public health and safety."
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#import-data",
    "href": "posts/Environmental Machine Learning Project/index.html#import-data",
    "title": "Environmental Data Project",
    "section": "Import Data",
    "text": "Import Data\nI obtained a dataset containing information on crop water requirement. There are 15 different types of crop. For each entry of crop, it’s described by the soil type (dry, humid or wet), a temperature range, weather condition, and region type. It also has details on the water requirement, which is the amount of water that a particular crop needs under certain environmental conditions to grow healthily. We want to predict whether irrigation is necessary. This is important because it can help farmers save time, money, water and also have a good environmental impact.\nhttps://www.kaggle.com/datasets/prateekkkumar/crop-water-requirement/data\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.read_csv('DATASET - Sheet1.csv')\n\ndf\n\n\n\n\n\n\n\n\nCROP TYPE\nSOIL TYPE\nREGION\nTEMPERATURE\nWEATHER CONDITION\nWATER REQUIREMENT\n\n\n\n\n0\nBANANA\nDRY\nDESERT\n10-20\nNORMAL\n8.750\n\n\n1\nBANANA\nDRY\nDESERT\n10-20\nSUNNY\n10.250\n\n\n2\nBANANA\nDRY\nDESERT\n10-20\nWINDY\n9.650\n\n\n3\nBANANA\nDRY\nDESERT\n10-20\nRAINY\n0.750\n\n\n4\nBANANA\nDRY\nDESERT\n20-30\nNORMAL\n9.850\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2875\nONION\nWET\nHUMID\n30-40\nRAINY\n0.100\n\n\n2876\nONION\nWET\nHUMID\n40-50\nNORMAL\n4.625\n\n\n2877\nONION\nWET\nHUMID\n40-50\nSUNNY\n6.125\n\n\n2878\nONION\nWET\nHUMID\n40-50\nWINDY\n5.625\n\n\n2879\nONION\nWET\nHUMID\n40-50\nRAINY\n0.200\n\n\n\n\n2880 rows × 6 columns"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#data-cleaning-and-prep",
    "href": "posts/Environmental Machine Learning Project/index.html#data-cleaning-and-prep",
    "title": "Environmental Data Project",
    "section": "Data Cleaning and Prep",
    "text": "Data Cleaning and Prep\nNow it’s time to prep the data. The first thing we want to do is transorm the temperature data from a range into something more readable with one digit. We can do this by creating a function that takes the middle value of the range and applying that to a new column called “TEMPERATURE_NUM”.\nThe next thing we want to do is to create one-hot encodings for the following columns: ‘CROP TYPE’, ‘SOIL TYPE’, ‘REGION’, ‘WEATHER CONDITION’. This is done to convert each category into a new binary (0 or 1) column. For example, for the column CROP TYPE_BANANA, if it is a banana then the value will be true, or otherwise false. By doing these one-hot encodings, it is easier for the machine to work with the data and understand the features of it.\nNow we want to create a column for our predictor data, which will be whether the data entry is bigger than or less than the median of the water requirement—- this will help us determine whether it needs irriation or not. We’re using the median of water requirement as our determinant for needing irrigation, because water requirement is the amount of water that a particular crop needs under certain environmental conditions to grow healthily. So if the water requirement is bigger smaller than the median then it needs irrigation since\n\ndef temp_to_num(temp_range):\n    low, high = temp_range.split('-')\n    return (float(low) + float(high)) / 2\n\ndf['TEMPERATURE_NUM'] = df['TEMPERATURE'].apply(temp_to_num)\n\ndf_encoded = pd.get_dummies(df, columns=['CROP TYPE', 'SOIL TYPE', 'REGION', 'WEATHER CONDITION'])\n\nmedian_req = df['WATER REQUIREMENT'].median()\ndf_encoded['needs_irrigation'] = (df['WATER REQUIREMENT'] &gt; median_req).astype(int)\n\ndf_encoded\n\n\n\n\n\n\n\n\n\nTEMPERATURE\nWATER REQUIREMENT\nneeds_irrigation\nTEMPERATURE_NUM\nCROP TYPE_BANANA\nCROP TYPE_BEAN\nCROP TYPE_CABBAGE\nCROP TYPE_CITRUS\nCROP TYPE_COTTON\nCROP TYPE_MAIZE\n...\nSOIL TYPE_HUMID\nSOIL TYPE_WET\nREGION_DESERT\nREGION_HUMID\nREGION_SEMI ARID\nREGION_SEMI HUMID\nWEATHER CONDITION_NORMAL\nWEATHER CONDITION_RAINY\nWEATHER CONDITION_SUNNY\nWEATHER CONDITION_WINDY\n\n\n\n\n0\n10-20\n8.750\n1\n15.0\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n1\n10-20\n10.250\n1\n15.0\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n2\n10-20\n9.650\n1\n15.0\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n3\n10-20\n0.750\n0\n15.0\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n4\n20-30\n9.850\n1\n25.0\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2875\n30-40\n0.100\n0\n35.0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n2876\n40-50\n4.625\n0\n45.0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n2877\n40-50\n6.125\n1\n45.0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n2878\n40-50\n5.625\n0\n45.0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n2879\n40-50\n0.200\n0\n45.0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n\n\n2880 rows × 30 columns"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#select-feature-columns-and-predictor-column.-then-split-data",
    "href": "posts/Environmental Machine Learning Project/index.html#select-feature-columns-and-predictor-column.-then-split-data",
    "title": "Environmental Data Project",
    "section": "Select feature columns and predictor column. Then split data",
    "text": "Select feature columns and predictor column. Then split data\nFirst we go through all the columns that we performed one-hot encoding on and the temperature, which will form the feature_cols, or input variables. Our y variable is needs_irrigation, which is the data we want to predict.\nThen we split the data. 20% will be used for testing, and the rest for training.\nLet’s look at the shape of our training data.\n\nfeature_cols = ['TEMPERATURE_NUM'] + \\\n               [col for col in df_encoded.columns if col.startswith(('CROP TYPE_', 'SOIL TYPE_', 'REGION_', 'WEATHER CONDITION_'))]\n\nX = df_encoded[feature_cols]\ny = df_encoded['needs_irrigation']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(f'Training samples: {len(X_train)}, Test samples: {len(X_test)}')\nprint('Features shape:', X_train.shape)\n\nTraining samples: 2304, Test samples: 576\nFeatures shape: (2304, 27)"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#decision-tree",
    "href": "posts/Environmental Machine Learning Project/index.html#decision-tree",
    "title": "Environmental Data Project",
    "section": "Decision Tree",
    "text": "Decision Tree\nHere we are using a decision tree. We chose a max depth of 7, which means the tree can have at most a depth of 7, or 7 levels. This is done to prevent overfitting, meaning a tree with too many levels might try too hard to capture all the details, leading to poorer acuracy.\nThen we fit the model onto our training data and predict whether the crop needs irrigation or not based on features like crop type, soil type, region, weather condition, and temperature.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\nclf = DecisionTreeClassifier(max_depth=7)\n\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#determine-accuracy",
    "href": "posts/Environmental Machine Learning Project/index.html#determine-accuracy",
    "title": "Environmental Data Project",
    "section": "Determine Accuracy",
    "text": "Determine Accuracy\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy}\")\n\nAccuracy: 0.8767361111111112\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import RocCurveDisplay\n\nRocCurveDisplay.from_estimator(clf, X_test, y_test)\nplt.show()"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#analysis-of-false-positive-rate-vs-true-positive-rate",
    "href": "posts/Environmental Machine Learning Project/index.html#analysis-of-false-positive-rate-vs-true-positive-rate",
    "title": "Environmental Data Project",
    "section": "Analysis of False Positive Rate vs True Positive Rate",
    "text": "Analysis of False Positive Rate vs True Positive Rate\nThis ROC curve is a graph that shows the performance of a binary classifier, and the closer it is to 1, the better it is at discrimination. The upward climb to 1 shows that the classifier is getting better and better at obtaining more accurate results."
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#random-forest",
    "href": "posts/Environmental Machine Learning Project/index.html#random-forest",
    "title": "Environmental Data Project",
    "section": "Random Forest",
    "text": "Random Forest\nLet’s try improving our performance by using Random Forest.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier(n_estimators=100, max_depth=7, random_state=42)\nrf_clf.fit(X_train, y_train)\nrf_pred = rf_clf.predict(X_test)\n\nprint(f\"Random Forest Test Accuracy: {accuracy_score(y_test, rf_pred):.4f}\")\n\nRandom Forest Test Accuracy: 0.9010"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#now-lets-visually-compare-the-accuracies",
    "href": "posts/Environmental Machine Learning Project/index.html#now-lets-visually-compare-the-accuracies",
    "title": "Environmental Data Project",
    "section": "Now let’s visually compare the accuracies",
    "text": "Now let’s visually compare the accuracies\nThey perform very similarly!\n\naccuracies = {\n    'Decision Tree': accuracy_score(y_test, y_pred),\n    'Random Forest': accuracy_score(y_test, rf_pred)\n}\n\nplt.bar(accuracies.keys(), accuracies.values())\nplt.ylim(0, 1)\nplt.ylabel('Accuracy')\nplt.title('Model Accuracy Comparison')\nplt.show()"
  },
  {
    "objectID": "posts/Environmental Machine Learning Project/index.html#importance-of-results",
    "href": "posts/Environmental Machine Learning Project/index.html#importance-of-results",
    "title": "Environmental Data Project",
    "section": "Importance of results",
    "text": "Importance of results\nIn this research project, we tried predict whether the crop needs irrigation or not based on features like crop type, soil type, region, weather condition, and temperature. We used a decision tree.\nUsing machine learning and data to determine whether a crop needs irrigation is important because it can be used to help the world: - save water (avoid overwatering and only use irrigation when needed) - environmental sustainability (reduce water run off and soil erosion) - save time and labor (help farmers use their resources wisely) - better crop health (too much water can be bad for the plant)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mlresearch",
    "section": "",
    "text": "Environmental Data Project\n\n\n\n\n\n\n\n\n\n\n\nApr 21, 2025\n\n\nMegan Tieu\n\n\n\n\n\n\nNo matching items"
  }
]